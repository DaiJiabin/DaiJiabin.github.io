[{"categories":["Learn"],"content":"Lecture by Andrew Ng, Coursera ","date":"2020-12-18","objectID":"/machine-learning/:0:0","tags":["AI"],"title":"Machine Learning","uri":"/machine-learning/"},{"categories":["Learn"],"content":"Week 1 Basic Concepts \u0026 Linear Regression ","date":"2020-12-18","objectID":"/machine-learning/:1:0","tags":["AI"],"title":"Machine Learning","uri":"/machine-learning/"},{"categories":["Learn"],"content":"Supervised Learning vs. Unsupervised Learning Supervised Learning: In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output. Training Data already has the value, which our Function should predict for a new, strange input. i.e, In the case below, we want to get a function, so that we can calculate the price of a house to be sold. We have 4 Training Data in this table and each Data has 4 Features that have a influence on its' price ( we call it as Multi-Features. It’ll be discussed later:) ). It’s devided in Regression and Classification Problems Size Number of Bedrooms Number of Floors Age(years) Price(1000$) $TrainingData_1$ 2104 5 1 45 460 $TrainingData_2$ 1416 3 2 40 232 $TrainingData_3$ 1534 3 2 30 315 $TrainingData_4$ 852 2 1 36 178 … … … … … … ($x_1$) ($x_2$) ($x_3$) ($x_4$) ($x_5$) Unsupervised Learning Unsupervised learning, on the other hand, allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don’t necessarily know the effect of the variables. ","date":"2020-12-18","objectID":"/machine-learning/:1:1","tags":["AI"],"title":"Machine Learning","uri":"/machine-learning/"},{"categories":["Learn"],"content":"Regression vs. Classification Regression: The Value we want to predict is continious instead of discrete. i.g., the Price of a House. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. Classification: As its' name, we want to devide the input in different classes. i.g., After training, with the help of the color, weight, outlook and smel ( Features ), we want to predict if the coffee beans we have come from Arabica or Robusta. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories. ","date":"2020-12-18","objectID":"/machine-learning/:1:2","tags":["AI"],"title":"Machine Learning","uri":"/machine-learning/"},{"categories":["Learn"],"content":"Linear Regression with 1 Variable / Feature Hypothesis Function $\\hat y = h_\\theta = \\theta_0 + \\theta_1x$ Cost Function $J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i=1}^m(\\hat y_i - y_i)^2$ $\\rarr J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i=1}^m(h_\\theta(x_i) - y_i)^2$ Optimize Gradient Descent. More specifically, repeat: $tmp_0 = \\theta_0 - \\alpha\\frac{\\partial}{\\theta_0}{J(\\theta_0, \\theta_1)}$ $\\rarr tmp_0 = \\theta_0 - \\alpha\\frac{1}{m}\\sum_{i=1}^m{(h_\\theta(x_i) - y_i)}$ $tmp_1 = \\theta_1 - \\alpha\\frac{\\partial}{\\theta_1}{J(\\theta_0, \\theta_1)}$ $\\rarr tmp_1 = \\theta_1 - \\alpha\\frac{1}{m}\\sum_{i=1}^m{((h_\\theta(x_i) - y_i)x_1)}$ $\\theta_0 = tmp_0$ $\\theta_1 = tmp_1$ We call $\\alpha$ Learning Rate. Its' value has influence on the speed, how fast we can find the parameters, with that we can reach a local optimal value. When $\\alpha$ too large / too small is: too large: we might miss the parameters, which can help us get the local optimal value. too small: We need more iterations ( Baby Steps / more time ). We can make the judge, whether $\\alpha$ too large / small ist by drawing the plot of iterations - $J(\\theta_0, \\theta_1)$: It should be lower. Otherwise is $\\alpha$ too large. We can start $\\alpha$ from 0.001, …, 0.01, …, 0.1. And Andrew Ng advises us to use 3 times ( 0.001, 0.003, 0.01, 0.03, 0.1 ) to try it. ","date":"2020-12-18","objectID":"/machine-learning/:1:3","tags":["AI"],"title":"Machine Learning","uri":"/machine-learning/"},{"categories":["Learn"],"content":"Multiple Features and Matrix Vector: A vector is a Matrix with one Column and many rows. i.g., $\\begin{bmatrix}x_0 \\\\ x_1 \\\\x_2 \\\\x_3 \\end{bmatrix}$ Hypothesis: We use the table above as example, we have 4 Training Data and each of them has 4 Features. According to this, we should have 4+1 parameters $\\theta_0-\\theta_4$. We write the Hypothesis below: $h_\\theta(x^i) = \\theta_0 + \\sum_{j=1}^4\\theta_jx_j^i$ $x_i^j$ here means the jth Feature of the ith Training Data. Actually we can add a Feature $x_0 \\equiv 1$, then we can use the Matrix. So that we can write the Equation above as below: $h_\\theta(x^i) = \\sum_{j=0}^4\\theta_jx_j^i$ We let: $\\Theta = \\begin{bmatrix}\\theta_0, \\theta_1, \\theta_2, \\theta_3, \\theta_4\\end{bmatrix}$, $X = \\begin{bmatrix} x^1_0 \u0026 x^2_0 \u0026 x^3_0 \u0026 x^4_0 \\\\ x^1_1 \u0026 x^2_1 \u0026 x^3_1 \u0026 x^4_1 \\\\ x^1_2 \u0026 x^2_2 \u0026 x^3_2 \u0026 x^4_2 \\\\ x^1_3 \u0026 x^2_3 \u0026 x^3_3 \u0026 x^4_3 \\\\ x^1_4 \u0026 x^2_4 \u0026 x^3_4 \u0026 x^4_4 \\end{bmatrix}$. So that $h_\\theta(x^i) = \\Theta * i_{th}$ column of $X$. In Matrix $X$, element $x_0^i \\equiv 1$. We can also calculate $\\Theta * X$ directly, the $i_{th}$ colum is the value of $h_\\theta(x^i)$. Cost Function: Assume we have m Training Data, each of them has n Features. $J(\\theta_0, \\theta_1, … \\theta_n) = \\frac{1}{2m}\\sum_{i=1}^m(h_\\theta(x^i) - y^i)^2$ Gradien Descent: repeat: $tmp_i = \\theta_i - \\alpha\\frac{\\partial}{\\partial\\theta_i}J(\\theta_0, \\theta_1, …, \\theta_n)$ $\\rarr tmp_i = \\theta_i - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_\\theta(x^i) - y^i)* x^i)$ $\\theta_i = tmp_i$ ","date":"2020-12-18","objectID":"/machine-learning/:1:4","tags":["AI"],"title":"Machine Learning","uri":"/machine-learning/"},{"categories":["Learn"],"content":"Feature Scaling We use this technique when the range of Features have great Difference. i.g., $0 \\leq x_1 \\leq 2000, 0 \\leq x_2 \\leq 5$, then we can $x_1 \\coloneqq \\frac{x_1}{2000}, x_2 \\coloneqq \\frac{x_2}{5}.$ This helps with the speed up of the Gradient Descent. Andrew Ng advices that scaling all Features approximately into the range [-1, 1]. We can also scale the Features by using this: $x_i = \\frac{x_i - \\mu_i}{s_i}$ Here $\\mu_i$ is the average value of Feature $x_i$, $s_i$ is normally $value_{max} - value_{min}$. ","date":"2020-12-18","objectID":"/machine-learning/:1:5","tags":["AI"],"title":"Machine Learning","uri":"/machine-learning/"},{"categories":["Learn"],"content":"Normal Equation The Method to solve for $\\Theta$ analytically: $\\Theta = (X^TX)^{-1}X^TY$ can minimize the $J(\\theta)$. $x^i = \\begin{bmatrix}x_0^i, x_1^i, …, x_n^i\\end{bmatrix}$ $X = \\begin{bmatrix} x_0^1 \u0026 x_1^1 \u0026 {…} \u0026 x_n^1 \\\\ x_0^2 \u0026 x_1^2 \u0026 {…} \u0026 x_n^2 \\\\ … \u0026 … \u0026 … \u0026 … \\\\x_0^m \u0026 x_1^m \u0026 {…} \u0026 x_n^m \\end{bmatrix}$ In this Matrix, we write each row the Features of one Training Data. The following table can helps us decide, when to use Normal Equation, when to use Gradient Descent. Normal Equation Gradient Descent No need to choose $\\alpha$. May have “Baby Step” issue, or miss the local optimal value when $\\alpha$ too large. Can be slow when n ( Number of Features ) too large is. Works fine even when n large is. Some Matrix is singular or degenerated. $n \\leq 10,000$ $n \u003e 10,000$ Reasons of uninvertibility of Matrix $(X^TX)^{-1}$: Redundant Elements: linear depent. i.g., $x_1 = $size in $feet^2$, $x_2$ = size in $m^2$. Too many Features: Just delete some of them, or use regulization ( discuss later ). ","date":"2020-12-18","objectID":"/machine-learning/:1:6","tags":["AI"],"title":"Machine Learning","uri":"/machine-learning/"},{"categories":["Learn"],"content":"Week 2 Logistic Regression ","date":"2020-12-18","objectID":"/machine-learning/:2:0","tags":["AI"],"title":"Machine Learning","uri":"/machine-learning/"},{"categories":["Learn"],"content":"Logistic Regression - Classification Examples: Email (Spam / Not Spam), Online Transactions: Fraudulent ( Yes / No ) ? $y \\isin {0, 1} \\begin{cases} 0 \u0026\\text{Negative Class} \\\\ 1 \u0026\\text{Positive Class} \\end{cases}$ p.s. In multi-class Prolems this set can have more than 2 elements. What if we use linear Regression in a classfication problem? $h_\\theta(x) = \\theta^Tx\u003c 0$ or $\u003e 1$ In Logistic Regression, it shouled always be: $0 \\leq h\\theta(x) \\leq 1$_. So we do the following transformation, then we get the Hypothesis: $h_\\theta(x) = g(\\theta^Tx)$ Then we use Sigmoid Function / Logistic Function: $g(z) = \\frac{1}{1 + e^{-z}}$ We have now: $h_\\theta(x) = \\frac{1}{1 + e ^ {-\\theta^Tx}}$ This Function $h_\\theta(x)$ comes out the result that estimated probability that $y = 1$ on input x. We can also write it in the following Form: $h_\\theta(x) = P(y = 1 | x; \\theta)$ $\\rarr$ Probability that $y = 1$, given $x$, parameterized by $\\theta$. $P(y = 1 | x; \\theta) + P(y = 0 | x; \\theta) = 1$ ","date":"2020-12-18","objectID":"/machine-learning/:2:1","tags":["AI"],"title":"Machine Learning","uri":"/machine-learning/"},{"categories":["Learn"],"content":"Deciding Boundary Linear Non-Linear ","date":"2020-12-18","objectID":"/machine-learning/:2:2","tags":["AI"],"title":"Machine Learning","uri":"/machine-learning/"},{"categories":["Learn"],"content":"Cost Function Training Set: ${(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), …, (x^{(m)}, y^{(m)})}$, m examples x $\\isin \\begin{bmatrix}x_0 \\\\ x_1 \\\\ … \\\\ x_m\\end{bmatrix}, x_0 \\equiv 1, y \\isin (0, 1)$ $h_\\theta(x) = \\frac{1}{1 + e ^ {-\\theta^Tx}}$ Brief Review: Cost Function in Linear Regression: $J(\\theta) = \\frac{1}{m}\\sum_{i=1}^m\\frac{1}{2}(h_\\theta(x^{(i)} - y^{(i)})^2$ $Cost(h_\\theta(x), y) = \\frac{1}{2}(h_\\theta(x) - y)^2$ $Cost(h_\\theta(x), y) = \\begin{cases}-log(h_\\theta(x))\u0026\\text{if y = 1}\\\\-log(1 - h_\\theta(x)) \u0026\\text{if y=0}\\end{cases}$ $Cost = 0$ if $y = 1, h_\\theta(x) = 0$. But as $h_\\theta(x) \\rarr 0, Cost \\rarr \\infty$ Predict $P(y=1|x; \\theta) = 0$, but $y = 1$, we’ll penalize learning algorithm by a very large cost. We can also write it like below: $Cost(h_\\theta(x), y) = -ylog(h_\\theta(x)) - (1-y)log(1 - h_\\theta(x))$ ","date":"2020-12-18","objectID":"/machine-learning/:2:3","tags":["AI"],"title":"Machine Learning","uri":"/machine-learning/"},{"categories":["Learn"],"content":"Gradient Descent $J(\\theta) = \\frac{1}{m}\\sum_{i=1}^mCost(h_\\theta(x^{(i)}), y^{(i)})$ $\\rarr J(\\theta) = -\\frac{1}{m}[\\sum_{i=1}^my^{(i)}log(h_\\theta(x^{(i)}) - y^{(i)}) + (1 - y^{(i)})log(1 - h_\\theta(x^{(i)}))]$ Repeat: $\\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)$ $\\rarr \\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j$ It looks identical to Linear Regression. BUT: $h_\\theta(x) = \\theta^Tx$ in Linear Regression In Logistic Regression is $h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^Tx}}$ Try to write it out: ","date":"2020-12-18","objectID":"/machine-learning/:2:4","tags":["AI"],"title":"Machine Learning","uri":"/machine-learning/"},{"categories":["Learn"],"content":"Anvanced Optimization Algorithms: Gradient Descent Conjugate gradient BFGS L-BFGS Advantages: No need to manually choose $\\alpha$ Often faster than gradient descent Disadvantages: More complex ","date":"2020-12-18","objectID":"/machine-learning/:2:5","tags":["AI"],"title":"Machine Learning","uri":"/machine-learning/"},{"categories":["Learn"],"content":"Multi-class Classification: one-vs-all ( one-vs-rest ) i.g., Email foldering / tagging: Work, Friends, Family, Hobby ($y=1, y=2, y=3, y=4$) $h_\\theta^{(i)} = P(y = i | x; \\theta)$ $(i=1,2,3)$ Train a logistic regression classifier $h_\\theta^{(i)}(x)$ for each class $i$ to predict the probability that $y=i$. On a new input $x$, to make a prediction, pick the class $i$ that __maximizes__ $max_ih_\\theta^{(i)}(x)$. ","date":"2020-12-18","objectID":"/machine-learning/:2:6","tags":["AI"],"title":"Machine Learning","uri":"/machine-learning/"},{"categories":null,"content":"About Me ","date":"2020-12-18","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Basic Infos Name: Dai Jiabin Birthday: 30.04.1997 Nation: China ","date":"2020-12-18","objectID":"/about/:1:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Contact Mobile ( Germany ): 015221429108 E-Mail: kechou.dai@gmail.com ","date":"2020-12-18","objectID":"/about/:2:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Academic Background 08.2015 - 06.2019 Bachelor in Xidian University, China, majored in Computer Science and Technology GPA: 3.6 Graduate Thesis: Design and Realize the CAPTCHA Recognization System based on Convolutional Neural Networks 10.2019 - 07.2020 Language ( German ) Learning in Germany, lectures provided by TUDIAS 10.2020 - 10.20203 ( assumed ) Master in Dresden University of Technology, Germany, majored in Informatik ","date":"2020-12-18","objectID":"/about/:3:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Awarded Status ","date":"2020-12-18","objectID":"/about/:4:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Abilities Language Field English CET4 576 CET6 551 German, GER C1.1 - C1.2 French Russian Spanish Italian Japanese Icelandic Programm Field Languages I’ve learned: Applications \u0026 Enviroments: Certificates on Coursera: Programm written by myself Social Field Other Field ","date":"2020-12-18","objectID":"/about/:5:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Lecture 1 ","date":"2020-12-18","objectID":"/nodejs/:0:0","tags":["NodeJS"],"title":"NodeJS","uri":"/nodejs/"},{"categories":null,"content":"What is NodeJS? Node.js is a JavaScript runtime built on Chrome’s V8 Node.js uses an event-driven, non-blocking I/O model Blocking: When I/O processing, process waits until I/O finishes. Non-blocking: When I/o processing, process doesn’t wait. Event-driven: i.g., Click event. ","date":"2020-12-18","objectID":"/nodejs/:1:0","tags":["NodeJS"],"title":"NodeJS","uri":"/nodejs/"},{"categories":null,"content":"Advantages of NodeJS Deals well with High Concurrency and I/O intensiv ","date":"2020-12-18","objectID":"/nodejs/:2:0","tags":["NodeJS"],"title":"NodeJS","uri":"/nodejs/"},{"categories":null,"content":"CPU Intensiv vs. I/O Intensiv CPU Intensiv: compress, decompress, encrpt, descrypt I/O Intensiv: Operations on File, Network and Database, which are offen used by Web ( Reading Static Resources, Operate Database, Render). ","date":"2020-12-18","objectID":"/nodejs/:3:0","tags":["NodeJS"],"title":"NodeJS","uri":"/nodejs/"},{"categories":null,"content":"How to deal with High Concurrency Increase Servers Increase the numbers of CPU-Cores And… NodeJS deals well with high Concurrency and I/O intensiv. ","date":"2020-12-18","objectID":"/nodejs/:4:0","tags":["NodeJS"],"title":"NodeJS","uri":"/nodejs/"},{"categories":null,"content":"NodeJS working Model Lecture 2 ","date":"2020-12-18","objectID":"/nodejs/:5:0","tags":["NodeJS"],"title":"NodeJS","uri":"/nodejs/"},{"categories":null,"content":"CommonJS, global and process CommonJS: Each file is a module, and has its own field. Inner the module, the parameter “module” represents module itself. Module.exports represents the API of module. modules.exports.\u003cVariable/Function\u003e = \u003cRename\u003e Rules of require() support extensions of js, json and node. Without path build-in Module or other third modules in node_modules will be used. Characters of require() In the 1st load module will be executed, and then stores in cache. When a module loaded repeatly, output the executed part only, instead of the unexecuted part. module.exports.test = 'A'; const modB = require('./05_modB'); console.log('modA: ', modB.test); module.exports.test = 'AA'; module.exports.test = 'B'; const modA = require('./05_modA'); console.log('modB: ', modA.test); module.exports.test = 'BB'; ","date":"2020-12-18","objectID":"/nodejs/:6:0","tags":["NodeJS"],"title":"NodeJS","uri":"/nodejs/"}]