<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <title>Machine Learning - Jiabin&#39;s Blog</title><meta name="Description" content="About LoveIt Theme"><meta property="og:title" content="Machine Learning" />
<meta property="og:description" content="Lecture by Andrew Ng, Coursera Basic Concepts &amp; Linear Regression Supervised Learning vs. Unsupervised Learning   Supervised Learning:
 In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.
   Training Data already has the value, which our Function should predict for a new, strange input." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://daijiabin.github.io/machine-learning/" />
<meta property="og:image" content="https://daijiabin.github.io/logo.png"/>
<meta property="article:published_time" content="2020-12-18T19:11:53+01:00" />
<meta property="article:modified_time" content="2020-12-30T08:45:51+01:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://daijiabin.github.io/logo.png"/>

<meta name="twitter:title" content="Machine Learning"/>
<meta name="twitter:description" content="Lecture by Andrew Ng, Coursera Basic Concepts &amp; Linear Regression Supervised Learning vs. Unsupervised Learning   Supervised Learning:
 In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.
   Training Data already has the value, which our Function should predict for a new, strange input."/>
<meta name="application-name" content="Jiabin&#39;s Blog">
<meta name="apple-mobile-web-app-title" content="Jiabin&#39;s Blog"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://daijiabin.github.io/machine-learning/" /><link rel="prev" href="https://daijiabin.github.io/nodejs/" /><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/normalize.css@8.0.1/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.7.2/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Machine Learning",
        "inLanguage": "en",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/daijiabin.github.io\/machine-learning\/"
        },"image": ["https:\/\/daijiabin.github.io\/images\/Apple-Devices-Preview.png"],"genre": "posts","keywords": "AI","wordcount":  5560 ,
        "url": "https:\/\/daijiabin.github.io\/machine-learning\/","datePublished": "2020-12-18T19:11:53+01:00","dateModified": "2020-12-30T08:45:51+01:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "Dai Jiabin","logo": "https:\/\/daijiabin.github.io\/images\/avatar.png"},"author": {
                "@type": "Person",
                "name": "Dai Jiabin"
            },"description": ""
    }
    </script></head>
    <body header-desktop="fixed" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Jiabin&#39;s Blog"><span class="header-title-pre"><i class='fas fa-user-secret fa-fw'></i></span><span id="id-1" class="typeit"></span></a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/categories/"> Categories </a><a class="menu-item" href="/about/"> About </a><span class="menu-item delimiter"></span><a href="javascript:void(0);" class="menu-item language" title="Select Language">English<i class="fas fa-chevron-right fa-fw"></i>
                        <select class="language-select" id="language-select-desktop" onchange="location = this.value;"><option value="/machine-learning/" selected>English</option><option value="/zh-cn/machine-learning/">简体中文</option></select>
                    </a><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Jiabin&#39;s Blog"><span class="header-title-pre"><i class='fas fa-user-secret fa-fw'></i></span><span id="id-2" class="typeit"></span></a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/categories/" title="">Categories</a><a class="menu-item" href="/about/" title="">About</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw"></i>
            </a><a href="javascript:void(0);" class="menu-item" title="Select Language">English<i class="fas fa-chevron-right fa-fw"></i>
                    <select class="language-select" onchange="location = this.value;"><option value="/machine-learning/" selected>English</option><option value="/zh-cn/machine-learning/">简体中文</option></select>
                </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animated flipInX">Machine Learning</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://www.github.com/DaiJiabin" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw"></i>Dai Jiabin</a></span>&nbsp;<span class="post-category">included in <a href="/categories/learn/"><i class="far fa-folder fa-fw"></i>Learn</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2020-12-18">2020-12-18</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;5560 words&nbsp;
                <i class="far fa-clock fa-fw"></i>&nbsp;27 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#basic-concepts--linear-regression">Basic Concepts &amp; Linear Regression</a>
      <ul>
        <li><a href="#supervised-learning-vs-unsupervised-learning">Supervised Learning vs. Unsupervised Learning</a></li>
        <li><a href="#regression-vs-classification">Regression vs. Classification</a></li>
        <li><a href="#linear-regression-with-1-variable--feature">Linear Regression with 1 Variable / Feature</a></li>
        <li><a href="#multiple-features-and-matrix">Multiple Features and Matrix</a></li>
        <li><a href="#feature-scaling">Feature Scaling</a></li>
        <li><a href="#normal-equation">Normal Equation</a></li>
      </ul>
    </li>
    <li><a href="#logistic-regression--overfitting--regularization">Logistic Regression &amp; Overfitting &amp; Regularization</a>
      <ul>
        <li><a href="#logistic-regression---classification">Logistic Regression - Classification</a></li>
        <li><a href="#deciding-boundary">Deciding Boundary</a></li>
        <li><a href="#cost-function">Cost Function</a></li>
        <li><a href="#gradient-descent">Gradient Descent</a></li>
        <li><a href="#anvanced-optimization">Anvanced Optimization</a></li>
        <li><a href="#multi-class-classification-one-vs-all--one-vs-rest-">Multi-class Classification: one-vs-all ( one-vs-rest )</a></li>
        <li><a href="#overfitting">Overfitting</a>
          <ul>
            <li><a href="#definiation">Definiation</a></li>
            <li><a href="#adressing-overfitting">Adressing Overfitting:</a></li>
          </ul>
        </li>
        <li><a href="#regularization">Regularization</a>
          <ul>
            <li><a href="#regularized-linear-regression">Regularized Linear Regression</a></li>
            <li><a href="#regularized-logistic-regression">Regularized Logistic Regression</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#non-linear-hypothesis---neural-networks">Non-Linear Hypothesis - Neural Networks</a>
      <ul>
        <li><a href="#neural-network">Neural Network</a>
          <ul>
            <li><a href="#presentations">Presentations:</a></li>
            <li><a href="#vectorized-implementation--forward-propagation">Vectorized Implementation &amp; Forward Propagation:</a></li>
          </ul>
        </li>
        <li><a href="#cost-function-1">Cost Function</a>
          <ul>
            <li><a href="#definitions">Definitions:</a></li>
            <li><a href="#classification-problems">Classification Problems:</a></li>
            <li><a href="#logistic-regression">Logistic Regression:</a></li>
          </ul>
        </li>
        <li><a href="#backpropagation-algorithm">Backpropagation Algorithm</a></li>
        <li><a href="#evaluating-hypothesis">Evaluating Hypothesis</a></li>
        <li><a href="#model-selection-training-validation-and-test">Model Selection, Training, Validation and Test</a>
          <ul>
            <li><a href="#model-selection">Model Selection</a></li>
            <li><a href="#train--books---validation--homework---test-error--examination-">Train ( Books ) / Validation ( Homework ) / Test Error ( Examination )</a></li>
            <li><a href="#diagnosing-bias--too-high---underfit--vs-variance--too-high---overfit-">Diagnosing Bias ( Too high -&gt; underfit ) vs. variance ( Too high -&gt; overfit )</a></li>
            <li><a href="#regularization-and-bias--variance">Regularization and bias / variance</a></li>
            <li><a href="#learning-curves">Learning Curves</a></li>
            <li><a href="#how-to-deal-with-high-bias--high-variance">How to deal with high Bias &amp; high Variance?</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#error-analysis">Error Analysis</a>
      <ul>
        <li><a href="#recommand-approach">Recommand Approach</a></li>
        <li><a href="#error-metrics-for-skewed-classes">Error metrics for skewed classes</a>
          <ul>
            <li><a href="#precision--recall">Precision / Recall</a></li>
            <li><a href="#trading-off-precision-and-recall">Trading off precision and recall</a></li>
            <li><a href="#f-score">F Score</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#support-vector-machines">Support Vector Machines</a>
      <ul>
        <li><a href="#optimization-objective">Optimization objective</a>
          <ul>
            <li><a href="#cost-function-2">Cost Function:</a></li>
            <li><a href="#svm-hypothesis">SVM Hypothesis:</a></li>
          </ul>
        </li>
        <li><a href="#large-margin-intuition">Large Margin Intuition</a>
          <ul>
            <li><a href="#svm-decision-boundary">SVM Decision Boundary</a></li>
          </ul>
        </li>
        <li><a href="#kernels">Kernels</a>
          <ul>
            <li><a href="#kernel-function">Kernel Function</a></li>
            <li><a href="#svm-with-kernels">SVM with Kernels</a></li>
          </ul>
        </li>
        <li><a href="#logistic-regression-vs-svms">Logistic Regression vs. SVMs</a></li>
      </ul>
    </li>
    <li><a href="#unsupervised-learning">Unsupervised Learning</a>
      <ul>
        <li><a href="#clustering">Clustering</a>
          <ul>
            <li><a href="#k-mean-algorithm">K-Mean Algorithm</a></li>
            <li><a href="#optimization-objective-of-k-mean-distortion-function">Optimization Objective of K-Mean: Distortion Function</a></li>
            <li><a href="#initialize-k-means">Initialize K-Means</a></li>
            <li><a href="#choosing-the-number-of-clusters">Choosing the number of clusters</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#dimentionality-reduction">Dimentionality Reduction</a>
      <ul>
        <li><a href="#pac-principal-component-analysis">PAC: Principal Component Analysis</a></li>
        <li><a href="#data-processing">Data Processing</a></li>
        <li><a href="#pca-algorithm">PCA algorithm</a></li>
        <li><a href="#choosing-k--number-of-principle-components-">Choosing k ( number of principle components )</a></li>
        <li><a href="#reconstruction-from-compressed-representation">Reconstruction from compressed representation</a></li>
        <li><a href="#design-of-ml-system-with-pca">Design of ML system with PCA</a></li>
      </ul>
    </li>
    <li><a href="#anomoly-detection">Anomoly Detection</a>
      <ul>
        <li><a href="#example-fraud-detection">Example: Fraud Detection</a></li>
        <li><a href="#gaussian-distribution">Gaussian Distribution</a></li>
        <li><a href="#algorithm">Algorithm</a>
          <ul>
            <li><a href="#evaluation">Evaluation</a></li>
          </ul>
        </li>
        <li><a href="#anomoly-detection-vs-supervised-learning">Anomoly Detection vs. Supervised Learning</a></li>
        <li><a href="#error-analysis-for-anomaly-detection">Error Analysis for anomaly detection</a></li>
        <li><a href="#multivariance-gaussian-distribution">Multivariance Gaussian Distribution</a></li>
      </ul>
    </li>
    <li><a href="#recommander-systems">Recommander Systems</a>
      <ul>
        <li><a href="#problem-formulation">Problem Formulation</a>
          <ul>
            <li><a href="#example-predicting-movie-ratings">Example: Predicting movie ratings</a></li>
          </ul>
        </li>
        <li><a href="#contnet-based-recommendations">Contnet-based recommendations</a>
          <ul>
            <li><a href="#optimization-algorithm">Optimization Algorithm</a></li>
          </ul>
        </li>
        <li><a href="#collaborative-filtering">Collaborative Filtering</a>
          <ul>
            <li><a href="#optimization-algorithm-1">Optimization Algorithm</a></li>
            <li><a href="#collaborative-filtering-1">Collaborative Filtering</a></li>
            <li><a href="#collaborative-filtering-algorithm">Collaborative Filtering Algorithm</a></li>
            <li><a href="#vectorization-low-rank-matrix-factorization">Vectorization: Low rank matrix factorization</a></li>
            <li><a href="#mean-normalization">Mean Normalization</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#learning-with-large-datasets">Learning with large datasets</a>
      <ul>
        <li><a href="#stochastic-gradient-descent">Stochastic gradient descent</a></li>
        <li><a href="#mini-batch-gradient-descent">Mini-batch gradient descent</a></li>
        <li><a href="#convergence">Convergence</a></li>
      </ul>
    </li>
    <li><a href="#at-the-end">At the End&hellip;</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/Machine-Learning.jpg"
        data-srcset="/Machine-Learning.jpg, /Machine-Learning.jpg 1.5x, /Machine-Learning.jpg 2x"
        data-sizes="auto"
        alt="/Machine-Learning.jpg"
        title="Machine-Learning" /></p>
<h1 id="lecture-by-andrew-ng-courserahttpswwwcourseraorglearnmachine-learning">Lecture by <a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="noopener noreffer">Andrew Ng, Coursera</a></h1>
<h2 id="basic-concepts--linear-regression">Basic Concepts &amp; Linear Regression</h2>
<h3 id="supervised-learning-vs-unsupervised-learning">Supervised Learning vs. Unsupervised Learning</h3>
<ul>
<li>
<p>Supervised Learning:</p>
<blockquote>
<p>In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.</p>
</blockquote>
<ul>
<li>
<p>Training Data already has the value, which our Function should predict for a new, strange input. i.e, In the case below, we want to get a function, so that we can calculate the price of a house to be sold. We have <strong>4 Training Data</strong> in this table and each Data has 4 <strong>Features</strong> that have a influence on its' price ( we call it as <strong>Multi-Features</strong>. It&rsquo;ll be discussed later:) ).</p>
</li>
<li>
<p>It&rsquo;s devided in <strong>Regression</strong> and <strong>Classification</strong> Problems</p>
</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">Size</th>
<th style="text-align:center">Number of Bedrooms</th>
<th style="text-align:center">Number of Floors</th>
<th style="text-align:center">Age(years)</th>
<th style="text-align:center">Price(1000$)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$TrainingData_1$</td>
<td style="text-align:center">2104</td>
<td style="text-align:center">5</td>
<td style="text-align:center">1</td>
<td style="text-align:center">45</td>
<td style="text-align:center">460</td>
</tr>
<tr>
<td style="text-align:center">$TrainingData_2$</td>
<td style="text-align:center">1416</td>
<td style="text-align:center">3</td>
<td style="text-align:center">2</td>
<td style="text-align:center">40</td>
<td style="text-align:center">232</td>
</tr>
<tr>
<td style="text-align:center">$TrainingData_3$</td>
<td style="text-align:center">1534</td>
<td style="text-align:center">3</td>
<td style="text-align:center">2</td>
<td style="text-align:center">30</td>
<td style="text-align:center">315</td>
</tr>
<tr>
<td style="text-align:center">$TrainingData_4$</td>
<td style="text-align:center">852</td>
<td style="text-align:center">2</td>
<td style="text-align:center">1</td>
<td style="text-align:center">36</td>
<td style="text-align:center">178</td>
</tr>
<tr>
<td style="text-align:center">&hellip;</td>
<td style="text-align:center">&hellip;</td>
<td style="text-align:center">&hellip;</td>
<td style="text-align:center">&hellip;</td>
<td style="text-align:center">&hellip;</td>
<td style="text-align:center">&hellip;</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">($x_1$)</td>
<td style="text-align:center">($x_2$)</td>
<td style="text-align:center">($x_3$)</td>
<td style="text-align:center">($x_4$)</td>
<td style="text-align:center">($x_5$)</td>
</tr>
</tbody>
</table>
<ul>
<li>
<p>Unsupervised Learning</p>
<blockquote>
<p>Unsupervised learning, on the other hand, allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don&rsquo;t necessarily know the effect of the variables.</p>
</blockquote>
</li>
</ul>
<h3 id="regression-vs-classification">Regression vs. Classification</h3>
<ul>
<li>
<p>Regression: The Value we want to predict is <strong>continious</strong> instead of discrete. E.g., the Price of a House.</p>
<blockquote>
<p>In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function.</p>
</blockquote>
</li>
<li>
<p>Classification: As its' name, we want to <strong>devide the input in different classes</strong>. E.g., After training, with the help of the <u>color, weight, outlook and smel</u> ( <strong>Features</strong> ), we want to predict if the coffee beans we have come from Arabica or Robusta.</p>
<blockquote>
<p>In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories.</p>
</blockquote>
</li>
</ul>
<h3 id="linear-regression-with-1-variable--feature">Linear Regression with 1 Variable / Feature</h3>
<ul>
<li>
<p>Hypothesis Function</p>
<ul>
<li>$\hat y = h_\theta = \theta_0 + \theta_1x$</li>
</ul>
</li>
<li>
<p>Cost Function</p>
<ul>
<li>
<p>$J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^m(\hat y_i - y_i)^2$</p>
<p>$\rarr J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^m(h_\theta(x_i) - y_i)^2$</p>
</li>
</ul>
</li>
<li>
<p>Optimize</p>
<ul>
<li>
<p><strong>Gradient Descent.</strong> More specifically, repeat:</p>
<p>$tmp_0 = \theta_0 - \alpha\frac{\partial}{\theta_0}{J(\theta_0, \theta_1)}$</p>
<p>$\rarr tmp_0 = \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^m{(h_\theta(x_i) - y_i)}$</p>
<p>$tmp_1 = \theta_1 - \alpha\frac{\partial}{\theta_1}{J(\theta_0, \theta_1)}$</p>
<p>$\rarr tmp_1 = \theta_1 - \alpha\frac{1}{m}\sum_{i=1}^m{((h_\theta(x_i) - y_i)x_1)}$</p>
<p>$\theta_0 = tmp_0$</p>
<p>$\theta_1 = tmp_1$</p>
<p>We call $\alpha$ <strong>Learning Rate.</strong> Its' value has influence on the speed, how fast we can find the parameters, with that we can reach a local optimal value.</p>
</li>
<li>
<p><strong>When $\alpha$ too large / too small is:</strong></p>
<ul>
<li>
<p>too large: we might miss the parameters, which can help us get the local optimal value.</p>
</li>
<li>
<p>too small: We need more iterations ( Baby Steps / more time ).</p>
</li>
<li>
<p>We can make the judge, whether $\alpha$ too large / small ist by drawing the plot of <strong>iterations - $J(\theta_0, \theta_1)$:</strong> It should be lower. Otherwise is $\alpha$ too large.</p>
</li>
<li>
<p>We can start $\alpha$ from <strong>0.001, &hellip;, 0.01, &hellip;, 0.1.</strong> And Andrew Ng advises us to use 3 times ( 0.001, 0.003, 0.01, 0.03, 0.1 ) to try it.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="multiple-features-and-matrix">Multiple Features and Matrix</h3>
<ul>
<li>
<p>Vector:</p>
<ul>
<li>A vector is a Matrix with <strong>one Column and many rows.</strong> E.g., $\begin{bmatrix}x_0 \\ x_1 \\x_2 \\x_3 \end{bmatrix}$</li>
</ul>
</li>
<li>
<p>Hypothesis:</p>
<ul>
<li>
<p>We use the table above as example, we have 4 Training Data and each of them has 4 Features. According to this, we should have 4+1 parameters $\theta_0-\theta_4$. We write the Hypothesis below:</p>
<p>$h_\theta(x^i) = \theta_0 + \sum_{j=1}^4\theta_jx_j^i$</p>
<p>$x_i^j$ here means the <em>jth Feature</em> of the <em>ith Training Data.</em></p>
<p>Actually we can add a Feature $x_0 \equiv 1$, then we can use the Matrix. So that we can write the Equation above as below:</p>
<p>$h_\theta(x^i) = \sum_{j=0}^4\theta_jx_j^i$</p>
<p>We let:</p>
<p>$\Theta = \begin{bmatrix}\theta_0, \theta_1, \theta_2, \theta_3, \theta_4\end{bmatrix}$,</p>
<p>$X = \begin{bmatrix}  x^1_0 &amp; x^2_0 &amp; x^3_0 &amp; x^4_0 \\ x^1_1 &amp; x^2_1 &amp; x^3_1 &amp; x^4_1 \\ x^1_2 &amp; x^2_2 &amp; x^3_2 &amp; x^4_2 \\ x^1_3 &amp; x^2_3 &amp; x^3_3 &amp; x^4_3 \\ x^1_4 &amp; x^2_4 &amp; x^3_4 &amp; x^4_4 \end{bmatrix}$.</p>
<p>So that $h_\theta(x^i) = \Theta * i_{th}$ column of $X$. In Matrix $X$, element $x_0^i \equiv 1$. We can also calculate $\Theta * X$ directly, the $i_{th}$ colum is the value of $h_\theta(x^i)$.</p>
</li>
</ul>
</li>
<li>
<p>Cost Function:</p>
<ul>
<li>
<p>Assume we have <strong>m Training Data</strong>, each of them has <strong>n Features</strong>.</p>
<p>$J(\theta_0, \theta_1, &hellip; \theta_n) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^i) - y^i)^2$</p>
</li>
</ul>
</li>
<li>
<p>Gradien Descent:</p>
<ul>
<li>
<p>repeat:</p>
<p>$tmp_i = \theta_i - \alpha\frac{\partial}{\partial\theta_i}J(\theta_0, \theta_1, &hellip;, \theta_n)$</p>
<p>$\rarr tmp_i = \theta_i - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^i) - y^i)* x^i)$</p>
<p>$\theta_i = tmp_i$</p>
</li>
</ul>
</li>
</ul>
<h3 id="feature-scaling">Feature Scaling</h3>
<ul>
<li>
<p>We use this technique <strong>when the range of Features have great Difference.</strong> E.g., $0 \leq x_1 \leq 2000, 0 \leq x_2 \leq 5$, then we can $x_1 \coloneqq \frac{x_1}{2000}, x_2 \coloneqq \frac{x_2}{5}.$ This helps with the speed up of the Gradient Descent.</p>
</li>
<li>
<p>Andrew Ng advices that scaling all Features <strong>approximately</strong> into the range <strong>[-1, 1]</strong>.</p>
</li>
<li>
<p>We can also scale the Features by using this:</p>
<p>$x_i = \frac{x_i - \mu_i}{s_i}$</p>
<p>Here $\mu_i$ is the average value of Feature $x_i$, $s_i$ is normally $value_{max} - value_{min}$.</p>
</li>
</ul>
<h3 id="normal-equation">Normal Equation</h3>
<ul>
<li>
<p>The Method to solve for $\Theta$ analytically:</p>
<p>$\Theta = (X^TX)^{-1}X^Ty$ can minimize the $J(\theta)$.</p>
<p>$x^i = \begin{bmatrix}x_0^i, x_1^i, &hellip;, x_n^i\end{bmatrix}$</p>
<p>$X = \begin{bmatrix} x_0^1 &amp; x_1^1 &amp; {&hellip;} &amp; x_n^1 \\ x_0^2 &amp; x_1^2 &amp; {&hellip;} &amp; x_n^2 \\ &hellip; &amp; &hellip; &amp; &hellip; &amp; &hellip; \\x_0^m &amp; x_1^m &amp; {&hellip;} &amp; x_n^m \end{bmatrix}$</p>
<p>In this Matrix, we write <strong>each row the Features of one Training Data</strong>.</p>
<p>$y = \begin{bmatrix}y^{(1)}\\&hellip;\\y^{(m)}\end{bmatrix}$</p>
</li>
<li>
<p>Suppose $m \leq n$, then is $X^TX$ is degenerated.</p>
</li>
<li>
<p>The following table can helps us decide, when to use <strong>Normal Equation</strong>, when to use <strong>Gradient Descent</strong>.</p>
<table>
<thead>
<tr>
<th style="text-align:center">Normal Equation</th>
<th style="text-align:center">Gradient Descent</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">No need to choose $\alpha$.</td>
<td style="text-align:center">May have &ldquo;Baby Step&rdquo; issue, or miss the local optimal value when $\alpha$ too large.</td>
</tr>
<tr>
<td style="text-align:center">Can be slow when <strong>n ( Number of Features )</strong> too large is.</td>
<td style="text-align:center">Works fine even when n large is.</td>
</tr>
<tr>
<td style="text-align:center">Some Matrix is <strong>singular or degenerated.</strong></td>
<td></td>
</tr>
<tr>
<td style="text-align:center">$n \leq 10,000$</td>
<td style="text-align:center">$n &gt; 10,000$</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>Reasons of uninvertibility of Matrix $(X^TX)^{-1}$:</p>
<ul>
<li>
<p>Redundant Elements: linear depent.</p>
<p>E.g., $x_1 = $size in $feet^2$, $x_2$ = size in $m^2$.</p>
</li>
</ul>
</li>
<li>
<p>Too many Features: Just delete some of them, or use <strong>regulization ( discuss later ).</strong></p>
</li>
</ul>
<h2 id="logistic-regression--overfitting--regularization">Logistic Regression &amp; Overfitting &amp; Regularization</h2>
<h3 id="logistic-regression---classification">Logistic Regression - Classification</h3>
<ul>
<li>
<p>Examples: Email (Spam / Not Spam), Online Transactions: Fraudulent ( Yes / No ) ?</p>
</li>
<li>
<p>$y \isin {0, 1} \begin{cases} 0 &amp;\text{Negative Class} \\ 1 &amp;\text{Positive Class} \end{cases}$</p>
</li>
<li>
<p>p.s. In <strong>multi-class</strong> Prolems this set can have more than 2 elements.</p>
</li>
<li>
<p>What if we use linear Regression in a classfication problem?</p>
<ul>
<li>$h_\theta(x) = \theta^Tx&lt; 0$ or $&gt; 1$</li>
</ul>
</li>
<li>
<p>In <strong>Logistic Regression</strong>, it shouled always be: <em><em>$0 \leq h</em>\theta(x) \leq 1$</em>_. So we do the following transformation, then we get the <strong>Hypothesis</strong>:</p>
<ul>
<li>
<p>$h_\theta(x) = g(\theta^Tx)$</p>
</li>
<li>
<p>Then we use <strong>Sigmoid Function / Logistic Function:</strong></p>
<ul>
<li>$g(z) = \frac{1}{1 + e^{-z}}$</li>
</ul>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/Sigmoid.png"
        data-srcset="/Sigmoid.png, /Sigmoid.png 1.5x, /Sigmoid.png 2x"
        data-sizes="auto"
        alt="/Sigmoid.png"
        title="Sigmoid" /></p>
</li>
<li>
<p>We have now:</p>
<ul>
<li>$h_\theta(x) = \frac{1}{1 + e ^ {-\theta^Tx}}$</li>
</ul>
</li>
<li>
<p>This Function $h_\theta(x)$ comes out the result that estimated probability that $y = 1$ on input x. We can also write it in the following Form:</p>
<p>$h_\theta(x) = P(y = 1 | x; \theta)$ $\rarr$ Probability that $y = 1$, given $x$, parameterized by $\theta$.</p>
</li>
<li>
<p>$P(y = 1 | x; \theta) + P(y = 0 | x; \theta) = 1$</p>
</li>
</ul>
</li>
</ul>
<h3 id="deciding-boundary">Deciding Boundary</h3>
<ul>
<li>
<p>Linear</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/Linear.png"
        data-srcset="/Linear.png, /Linear.png 1.5x, /Linear.png 2x"
        data-sizes="auto"
        alt="/Linear.png"
        title="Linear" /></p>
</li>
<li>
<p>Non-Linear</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/Non-Linear.png"
        data-srcset="/Non-Linear.png, /Non-Linear.png 1.5x, /Non-Linear.png 2x"
        data-sizes="auto"
        alt="/Non-Linear.png"
        title="Non-Linear" /></p>
</li>
</ul>
<h3 id="cost-function">Cost Function</h3>
<ul>
<li>
<p>Training Set: ${(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), &hellip;, (x^{(m)}, y^{(m)})}$, <strong>m examples</strong></p>
</li>
<li>
<p>x $\isin \begin{bmatrix}x_0 \\ x_1 \\ &hellip; \\ x_m\end{bmatrix}, x_0 \equiv 1, y \isin (0, 1)$</p>
</li>
<li>
<p>$h_\theta(x) = \frac{1}{1 + e ^ {-\theta^Tx}}$</p>
<blockquote>
<p>Brief Review: Cost Function in <strong>Linear Regression:</strong><br>
$J(\theta) = \frac{1}{m}\sum_{i=1}^m\frac{1}{2}(h_\theta(x^{(i)} - y^{(i)})^2$<br>
$Cost(h_\theta(x), y) = \frac{1}{2}(h_\theta(x) - y)^2$</p>
</blockquote>
</li>
<li>
<p>$Cost(h_\theta(x), y) = \begin{cases}-log(h_\theta(x))&amp;\text{if y = 1}\\-log(1 - h_\theta(x)) &amp;\text{if y=0}\end{cases}$</p>
<ul>
<li>
<p>if $y = 1, h_\theta(x) = 1 \rarr Cost = 0$.</p>
</li>
<li>
<p>But as $h_\theta(x) \rarr 0, Cost \rarr \infty$</p>
<p>Predict $P(y=1|x; \theta) = 0$, but $y = 1$, we&rsquo;ll penalize learning algorithm by a very large cost.</p>
</li>
</ul>
</li>
<li>
<p>We can also write it like below:</p>
<ul>
<li>$Cost(h_\theta(x), y) = -ylog(h_\theta(x)) - (1-y)log(1 - h_\theta(x))$</li>
</ul>
</li>
</ul>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/Logistic-Regression-cost-function.png"
        data-srcset="/Logistic-Regression-cost-function.png, /Logistic-Regression-cost-function.png 1.5x, /Logistic-Regression-cost-function.png 2x"
        data-sizes="auto"
        alt="/Logistic-Regression-cost-function.png"
        title="Logistic-Cost-Function" /></p>
<h3 id="gradient-descent">Gradient Descent</h3>
<ul>
<li>
<p>$J(\theta) = \frac{1}{m}\sum_{i=1}^mCost(h_\theta(x^{(i)}), y^{(i)})$</p>
<p>$\rarr J(\theta) = -\frac{1}{m}[\sum_{i=1}^my^{(i)}log(h_\theta(x^{(i)}) - y^{(i)}) + (1 - y^{(i)})log(1 - h_\theta(x^{(i)}))]$</p>
</li>
<li>
<p>Repeat:</p>
<p>$\theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta)$</p>
<p>$\rarr \theta_j := \theta_j - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x^{(i)}_j$</p>
<p><strong>It looks identical to Linear Regression. BUT:</strong></p>
<ul>
<li>
<p>$h_\theta(x) = \theta^Tx$ in Linear Regression</p>
</li>
<li>
<p>In Logistic Regression is $h_\theta(x) = \frac{1}{1 + e^{-\theta^Tx}}$</p>
</li>
</ul>
</li>
<li>
<p><strong>Try to write it out:</strong></p>
</li>
</ul>
<h3 id="anvanced-optimization">Anvanced Optimization</h3>
<ul>
<li>
<p>Algorithms:</p>
<ul>
<li>
<p><del>Gradient Descent</del></p>
</li>
<li>
<p>Conjugate gradient</p>
</li>
<li>
<p>BFGS</p>
</li>
<li>
<p>L-BFGS</p>
</li>
</ul>
<p>Advantages:</p>
<ul>
<li>
<p>No need to manually choose $\alpha$</p>
</li>
<li>
<p>Often faster than gradient descent</p>
</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>More complex</li>
</ul>
</li>
</ul>
<h3 id="multi-class-classification-one-vs-all--one-vs-rest-">Multi-class Classification: one-vs-all ( one-vs-rest )</h3>
<ul>
<li>
<p>E.g., Email foldering / tagging: Work, Friends, Family, Hobby ($y=1, y=2, y=3, y=4$)</p>
</li>
<li>
<p>$h_\theta^{(i)} = P(y = i | x; \theta)$ $(i=1,2,3)$</p>
</li>
<li>
<p>Train a logistic regression classifier $h_\theta^{(i)}(x)$ for each class $i$ to predict the probability that $y=i$. On a new input $x$, to make a prediction, pick the class $i$ that maximizes $max_ih_\theta^{(i)}(x)$.</p>
</li>
</ul>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/multiclass.png"
        data-srcset="/multiclass.png, /multiclass.png 1.5x, /multiclass.png 2x"
        data-sizes="auto"
        alt="/multiclass.png"
        title="multiclass.png" /></p>
<h3 id="overfitting">Overfitting</h3>
<h4 id="definiation">Definiation</h4>
<ul>
<li>If we have too many Features, the learned hypothesis may <strong>fit the training set very well, but fail to generalize to new examples</strong>. $\darr$</li>
</ul>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/Overfitting.png"
        data-srcset="/Overfitting.png, /Overfitting.png 1.5x, /Overfitting.png 2x"
        data-sizes="auto"
        alt="/Overfitting.png"
        title="Overfitting.png" /></p>
<ul>
<li>On Logistic Regression $\darr$</li>
</ul>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/Overfitting-Logistic.png"
        data-srcset="/Overfitting-Logistic.png, /Overfitting-Logistic.png 1.5x, /Overfitting-Logistic.png 2x"
        data-sizes="auto"
        alt="/Overfitting-Logistic.png"
        title="Overfitting-Logistic.png" /></p>
<h4 id="adressing-overfitting">Adressing Overfitting:</h4>
<ol>
<li>
<p><strong>Reduce number of Features</strong></p>
<ul>
<li>
<p>Manually select which Features to keep</p>
</li>
<li>
<p>Model selection algorithm</p>
</li>
</ul>
</li>
<li>
<p><strong>Regularization</strong></p>
<ul>
<li>
<p>Keep all the Features, but reduce magnitude / values of parameters $\theta$.</p>
</li>
<li>
<p>Works well when we have a lot of Features, each of which contributes a bit to predicting $y$.</p>
</li>
</ul>
</li>
</ol>
<h3 id="regularization">Regularization</h3>
<ul>
<li>
<p>Small Values for parameters $\theta_0, \theta_1, &hellip;, \theta_n$</p>
<ul>
<li>
<p>&ldquo;Simpler&rdquo; hypothesis</p>
</li>
<li>
<p>Less prone to overfitting</p>
</li>
</ul>
</li>
<li>
<p>E.g. Housing:</p>
<ul>
<li>
<p>Features: $x_1, x_2, &hellip;, x_{100}$</p>
</li>
<li>
<p>Parameters: $\theta_0, \theta_1, &hellip;, \theta_n$</p>
</li>
<li>
<p>$J(\theta) = \frac{1}{2m}[\sum_{i=1}^m(h_\theta(x^{(i)} - y^{(i)})^2 + \lambda\sum_{j=1}^{n}\theta_j^2]$. We pinelize only $\theta_{i &gt; 0}$</p>
</li>
<li>
<p>$\lambda\sum_{j=1}^{n}\theta_j^2$ is called Regulization Parameter.</p>
</li>
<li>
<p>When the $\lambda$ too large is, $h_\theta(x) = \theta_0$ (Because this can make $\theta_{i &gt; 0} \thickapprox 0$)</p>
</li>
</ul>
</li>
</ul>
<h4 id="regularized-linear-regression">Regularized Linear Regression</h4>
<ul>
<li>
<p><strong>Cost Function:</strong></p>
<ul>
<li>$J(\theta) = \frac{1}{2m}[\sum_{i=1}^m(h_\theta(x^{(i)} - y^{(i)})^2 + \lambda\sum_{j=1}^{n}\theta_j^2]$.</li>
</ul>
</li>
<li>
<p><strong>Gradient Descent:</strong></p>
<ul>
<li>
<p>$\theta_0 := \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})(x_0 \equiv 1))$</p>
</li>
<li>
<p>$\theta_{j &gt; 0} := \theta_j - \alpha[\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)} - y^{(i)}))x^{(i)} + \frac{\lambda}{m}\theta_j]$</p>
<p>$\rarr \theta_{j &gt; 0} := \theta_j(1- \alpha\frac{\lambda}{m}) - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)} - y^{(i)}))x^{(i)}$</p>
</li>
<li>
<p>In the Equation above, $ \theta_j(1- \alpha\frac{\lambda}{m}) &lt; 1$. We can be sure that the parameter will be smaller in each iteration.</p>
</li>
</ul>
</li>
<li>
<p><strong>Normal Equition:</strong></p>
<ul>
<li>
<p>$X = \begin{bmatrix}(x^{(i)})^T\\&hellip;\\(x^{(m)})^T\\\end{bmatrix}$</p>
</li>
<li>
<p>$y = \begin{bmatrix}y^{(1)}\\&hellip;\\y^{(m)}\end{bmatrix}$</p>
</li>
<li>
<p>$\theta = (X^TX + \lambda\begin{bmatrix}0 &amp; 0 &amp; 0\\0 &amp; 1 &amp;0 \\ 0 &amp; 0 &amp; 1\end{bmatrix})^{-1}X^Ty$</p>
<ul>
<li>The Elements in the Matrix next to $\lambda$ looks like: $e_{i = j\not = 0}$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="regularized-logistic-regression">Regularized Logistic Regression</h4>
<ul>
<li>
<p><strong>Cost Function:</strong></p>
<ul>
<li>$J(\theta) = -[\frac{1}{m}\sum_{i=1}^my^{(i)}logh_\theta(x^{(i)}) + (1 - y^{(i)})log(1 - h_\theta(x^{(i)}))] + \frac{\lambda}{2m}\sum_{j=1}^m\theta_j^2$</li>
</ul>
</li>
<li>
<p><strong>Gradient Descent:</strong></p>
<ul>
<li>
<p>$\theta_0 := \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})(x_0 \equiv 1))$</p>
</li>
<li>
<p>$\theta_{j &gt; 0} := \theta_j - \alpha[\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)} - y^{(i)}))x^{(i)} + \frac{\lambda}{m}\theta_j]$</p>
</li>
<li>
<p>Here $h_\theta(x) = \frac{1}{1 + e^{-\theta^Tx}}$</p>
</li>
</ul>
</li>
</ul>
<h2 id="non-linear-hypothesis---neural-networks">Non-Linear Hypothesis - Neural Networks</h2>
<ul>
<li>
<p>What should we do when we proceed an Image?</p>
<ul>
<li>Transfer it into a grayscale-image. It has only $\frac{1}{3}$ Features that RGB-image has.</li>
</ul>
</li>
</ul>
<h3 id="neural-network">Neural Network</h3>
<ul>
<li>
<p>Origins: Algorithms that try to mimic the brain.</p>
</li>
<li>
<p>Neuron Model: Logistic Unit</p>
<p>$x = \begin{bmatrix}x_0\\x_1\\&hellip;\\x_n\end{bmatrix}$ (Just a Vector of Features.)</p>
<p>$\theta = \begin{bmatrix}\theta_0\\\theta_1\\&hellip;\\\theta_n\end{bmatrix}$</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/Logistic%20Unit.jpg"
        data-srcset="/Logistic%20Unit.jpg, /Logistic%20Unit.jpg 1.5x, /Logistic%20Unit.jpg 2x"
        data-sizes="auto"
        alt="/Logistic%20Unit.jpg"
        title="Logistic Unit.jpg" /></p>
<p>In this Picture, $x_0$ is called <strong>the bias unit</strong> or <strong>the bias neuron.</strong> And $x_0 \equiv 1$</p>
<p>There&rsquo;s a <strong>sigmoid(logistic) Activaition Function</strong> in this Picture</p>
</li>
</ul>
<h4 id="presentations">Presentations:</h4>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/NN.png"
        data-srcset="/NN.png, /NN.png 1.5x, /NN.png 2x"
        data-sizes="auto"
        alt="/NN.png"
        title="NN.png" /></p>
<ul>
<li>
<p>$a_i^{(j)}: $ Activation of Unit $i$ in Layer $j$</p>
</li>
<li>
<p>$\Theta^{(j)}: $ Matrix of weights controlling function mapping from layer $j$ to layer $j+1$</p>
</li>
<li>
<p>⚠️ If networks has $s_j$ units in layer $j$, $s_{j+1}$ Units in layer $j+1$, then $\Theta^{(j)}$ will be of dimension $s_{j+1} \times (s_j + 1)$.
__And it should be randomly intialized.__</p>
</li>
</ul>
<h4 id="vectorized-implementation--forward-propagation">Vectorized Implementation &amp; Forward Propagation:</h4>
<ul>
<li>
<p>$x = \begin{bmatrix}x_0\\x_1\\x_2\\x_3\end{bmatrix}$</p>
<p>$z^{(2)} = \begin{bmatrix}z_1^{(2)}\\z_2^{(2)}\\z_3^{(2)}\end{bmatrix}$</p>
<p>$z^{(2)} = \Theta^{(1)}x$</p>
<ul>
<li>
<p>We can image Input Layer as $a^{(1)}$. Then we can write in this Form:</p>
<p>$z^{(2)} = \Theta^{(1)}a^{(1)}$</p>
</li>
</ul>
<p>$a^{(2)} = g(z^{(2)})$ (Here $g$ is the Sigmoid Function.)</p>
<ul>
<li>Add $a_0^{(2)} = 0$</li>
</ul>
<p>$z^{(3)} = \Theta^{(2)}a^{(2)}$</p>
<p>$h_\theta(x) = a^{(3)} = g(z^{(3)})$</p>
</li>
<li>
<p>We can through choosing $\theta$ to realize <em>AND, OR, NOT, XOR, XNOR</em>&hellip;</p>
</li>
</ul>
<h3 id="cost-function-1">Cost Function</h3>
<h4 id="definitions">Definitions:</h4>
<ul>
<li>
<p>Training Datas: ${(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), &hellip;, (x^{(m)}, y^{(m)})}$</p>
</li>
<li>
<p>$L = $ total no. of Layers in network</p>
</li>
<li>
<p>$s_l = $ no. of units ( not counting bias unit ) in layer $l$.</p>
</li>
</ul>
<h4 id="classification-problems">Classification Problems:</h4>
<table>
<thead>
<tr>
<th style="text-align:center">Binary Classification</th>
<th style="text-align:center">Multi-class Classification ( K classes )</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$y = 0$ or $1$</td>
<td style="text-align:center">$y \isin \R^K$, E.g. $\begin{bmatrix}1\\0\\0\\0\end{bmatrix}$, $\begin{bmatrix}0\\1\\0\\0\end{bmatrix}$, $\begin{bmatrix}0\\0\\1\\0\end{bmatrix}$, $\begin{bmatrix}0\\0\\0\\1\end{bmatrix}$</td>
</tr>
<tr>
<td style="text-align:center">1 output unit</td>
<td style="text-align:center">K output units</td>
</tr>
</tbody>
</table>
<h4 id="logistic-regression">Logistic Regression:</h4>
<blockquote>
<p>Brief Review:<br>
$J(\theta) = -\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}log(h_\theta(x^{(i)})) + (1 - y^{(i)})log(1 - h_\theta(x^{(i)}))] + \frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2$</p>
</blockquote>
<ul>
<li>
<p>Neural Network:</p>
<p>$h_\Theta(x) \isin \R^K, (h_\Theta(x))_i = i^{th}$ Output</p>
<p>$J(\Theta) = -\frac{1}{m}[\sum_{i=1}^{m}\sum_{k=1}^{K}y_k^{(i)}log(h_\Theta(x^{(i)}))_k + (1 - y_k^{(i)})log(1 - (h_\Theta(x^{(i)}))_k)] + \frac{\lambda}{2m}\sum_{l = 1}^{L - 1}\sum_{i = 1}^{s_l}\sum_{j = 1}^{s_{l+1}}(\Theta_{ji}^{l})^2$</p>
</li>
</ul>
<h3 id="backpropagation-algorithm">Backpropagation Algorithm</h3>
<ul>
<li>
<p>Gradiet Descent, we need to compute:</p>
<ul>
<li>
<p>$J(\Theta)$</p>
</li>
<li>
<p>$\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)$</p>
</li>
</ul>
<blockquote>
<p>$J(\Theta) = -\frac{1}{m}[\sum_{i=1}^{m}\sum_{k=1}^{K}y_k^{(i)}log(h_\Theta(x^{(i)}))_k + (1 - y_k^{(i)})log(1 - (h_\Theta(x^{(i)}))_k)] + \frac{\lambda}{2m}\sum_{l = 1}^{L - 1}\sum_{i = 1}^{s_l}\sum_{j = 1}^{s_{l+1}}(\Theta_{ji}^{l})^2$</p>
</blockquote>
</li>
<li>
<p>Intuition:</p>
<ul>
<li>
<p>$\delta_j^{(l)}$ = &ldquo;error&rdquo; of node $j$ in layer $l$.</p>
</li>
<li>
<p>For each output Unit ( Layer = 4 ):</p>
<p>$\delta_j^{(4)} = a_j^{(4)} - y_j$</p>
<p>$a_j^{(4)} = (h_\theta(x))_j$</p>
<p>We have: $\delta^{(4)} = a^{(4)} - y$</p>
</li>
<li>
<p>Then we execute the following Operations:</p>
<p>$\delta^{(3)} = (\Theta^{(3)})^T\delta^{(4)} .* g'(z^{(3)})$</p>
<p>$\delta^{(2)} = (\Theta^{(2)})^T\delta^{(3)} .* g'(z^{(2)})$</p>
<p>Here $g'(z^{(n)}) = a^{(n)} .* (1 - a^{(n)})$</p>
</li>
</ul>
</li>
<li>
<p>Specifically:</p>
<ul>
<li>
<p>Training Set: ${(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), &hellip;, (x^{(m)}, y^{(m)})}$</p>
</li>
<li>
<p>Set $\Delta_{ij}^{(l)} = 0$, for all $i, j, l$</p>
</li>
<li>
<p>For $i = 1$ to $m$:</p>
<p>Set $a^{(1)} = x^{(i)}$</p>
<p>Perform forward propagation to compute $a^{(l)}$ for $l = 2, 3, &hellip;, L$</p>
<p>Using $y^{(i)}$, compute $\delta^{(L)} = a^{(L)} - y^{(i)}$</p>
<p>Compute $\delta^{(L - 1)}, \delta^{(L - 2)}, &hellip;, \delta^{(2)}$</p>
<p>$\Delta_{ij}^{(l)}:= \Delta_{ij}^{(l)} + a_j^{(l)}\delta_i^{(l+1)}$</p>
</li>
<li>
<p>$D_{ij}^{(l)} := \frac{1}{m}\Delta_{ij}^{(l)} + \lambda\Theta_{ij}^{(l)}$ if $j \neq 0$</p>
</li>
<li>
<p>$D_{ij}^{(l)} := \frac{1}{m}\Delta_{ij}^{(l)}$ if $j = 0$</p>
</li>
</ul>
</li>
</ul>
<h3 id="evaluating-hypothesis">Evaluating Hypothesis</h3>
<ul>
<li>
<p>Split the labeled data <strong>randomly</strong> into 2 Parts: Training Set ( 70% ) and Test Set ( 30% ).</p>
</li>
<li>
<p>Learn the parameter $\theta$ from Training data</p>
</li>
<li>
<p>Computing test Set error:</p>
<ul>
<li>
<p>In Linear Regression: $J(\theta) = \frac{1}{2m_{test}}\sum_{i = 1}^{m_{test}}(J(x_{test}^{(i)}) - y_{test}^{(i)})^2$</p>
</li>
<li>
<p>In Logistic Regression: $J(\theta) = =-\frac{1}{m_{test}}\sum_{i = 1}^{m_{test}}[y^{(i)}logh_{\theta}(x^{(i)}_{test}) + (1 - y^{(i)})log(1 - h_{\theta}(x^{(i)}_{test}))]$</p>
</li>
</ul>
</li>
<li>
<p>Misclassification error ( 0/1 multiclassification error ):</p>
<ul>
<li>$err(h_{\theta}(x), y) = 1$, if $h_{\theta}(x) \leq 0.5, y = 1 | h_{\theta}(x) \geq 0.5, y = 0$</li>
<li>$Test_{error} = \frac{1}{m_{test}}\sum_{i = 1}^{m_{test}}err(h_{\theta}(x_{test}^{(i)}), y^{(i)})$</li>
</ul>
</li>
</ul>
<h3 id="model-selection-training-validation-and-test">Model Selection, Training, Validation and Test</h3>
<h4 id="model-selection">Model Selection</h4>
<p>1). $h_{\theta}(x) = \theta_0 + \theta_1(x) \rarr \theta^{(1)} \rarr J_{test}(\theta^{(1)})$</p>
<p>2). $h_{\theta}(x) = \theta_0 + \theta_1(x) + \theta_2^2(x) \rarr \theta^{(2)} \rarr J_{test}(\theta^{(2)})$</p>
<p>3). $h_{\theta}(x) = \theta_0 + \theta_1(x) + \theta_2^2(x) + \theta_3^3(x) \rarr \theta^{(3)} \rarr J_{test}(\theta^{(3)})$</p>
<p>&hellip;</p>
<p>10). $h_{\theta}(x) = \theta_0 + \theta_1(x) + \theta_2^2(x) + \theta_3^3(x) + &hellip; + \theta_{10}^{10}(x) \rarr \theta^{(10)} \rarr J_{test}(\theta^{(10)})$</p>
<ul>
<li>
<p>We should&rsquo;t use the chosen $\theta^{(5)}$ to test how well it fits in our Test Set. Because we become it from Training Set, and the Test Set has the same Dimension as the Training Set. <strong>It lacks of Generalization on New Data Set.</strong></p>
</li>
<li>
<p>Thus, we split the Data Set into 3 pieces: <strong>Training Set ( 60% ), Cross Validation Set ( 20% ), Test Set ( 20% )</strong>.</p>
</li>
</ul>
<h4 id="train--books---validation--homework---test-error--examination-">Train ( Books ) / Validation ( Homework ) / Test Error ( Examination )</h4>
<ul>
<li>
<p>Training Error</p>
<ul>
<li>$J_{train}(\theta) = \frac{1}{2m_{train}}\sum_{i = 1}^{m_{train}}(h_{\theta}(x_{train}^{(i)}) - y_{train}^{(i)})^2$</li>
</ul>
</li>
<li>
<p>Validation Error</p>
<ul>
<li>$J_{cv}(\theta) = \frac{1}{2m_{cv}}\sum_{i = 1}^{m_{cv}}(h_{\theta}(x_{cv}^{(i)}) - y_{cv}^{(i)})^2$</li>
</ul>
</li>
<li>
<p>Test Error</p>
<ul>
<li>$J_{test}(\theta) = \frac{1}{2m_{test}}\sum_{i = 1}^{m_{test}}(h_{\theta}(x_{test}^{(i)}) - y_{test}^{(i)})^2$</li>
</ul>
<p>1). $h_{\theta}(x) = \theta_0 + \theta_1(x) \rarr \theta^{(1)} \rarr J_{cv}(\theta^{(1)})$</p>
<p>2). $h_{\theta}(x) = \theta_0 + \theta_1(x) + \theta_2^2(x) \rarr \theta^{(2)} \rarr J_{cv}(\theta^{(2)})$</p>
<p>3). $h_{\theta}(x) = \theta_0 + \theta_1(x) + \theta_2^2(x) + \theta_3^3(x) \rarr \theta^{(3)} \rarr J_{cv}(\theta^{(3)})$</p>
<p>&hellip;</p>
<p>10). $h_{\theta}(x) = \theta_0 + \theta_1(x) + \theta_2^2(x) + \theta_3^3(x) + &hellip; + \theta_{10}^{10}(x) \rarr \theta^{(10)} \rarr J_{cv}(\theta^{(10)})$</p>
<p>Pick $h_{\theta}(x) = \theta_0 + \theta_1(x) + \theta_2^2(x) + \theta_3^3(x) + \theta_4^4(x)$, Estimate generalization error for test set $J_{test}(\theta^{(4)})$</p>
</li>
</ul>
<h4 id="diagnosing-bias--too-high---underfit--vs-variance--too-high---overfit-">Diagnosing Bias ( Too high -&gt; underfit ) vs. variance ( Too high -&gt; overfit )</h4>
<ul>
<li>
<p>Training Error</p>
<ul>
<li>$J_{train}(\theta) = \frac{1}{2m_{train}}\sum_{i = 1}^{m_{train}}(h_{\theta}(x_{train}^{(i)}) - y_{train}^{(i)})^2$</li>
</ul>
</li>
<li>
<p>Validation Error</p>
<ul>
<li>$J_{cv}(\theta) = \frac{1}{2m_{test}}\sum_{i = 1}^{m_{cv}}(h_{\theta}(x_{cv}^{(i)}) - y_{cv}^{(i)})^2$</li>
</ul>
</li>
<li>
<p>Draw the <strong>&ldquo;Degree of polynomioal d - Error&rdquo; plot.</strong> In this Plot draw the $J_{train}(\theta), J_{cv/test}(\theta)$. Then we can make sure wether it&rsquo;s high bias or high variance.</p>
</li>
</ul>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/Bias%20and%20Variance.jpeg"
        data-srcset="/Bias%20and%20Variance.jpeg, /Bias%20and%20Variance.jpeg 1.5x, /Bias%20and%20Variance.jpeg 2x"
        data-sizes="auto"
        alt="/Bias%20and%20Variance.jpeg"
        title="Bias and Variance" /></p>
<ul>
<li>
<p>Bias ( Underfit, when d too small ):</p>
<ul>
<li>
<p>$J_{train}(\theta)$ will be high</p>
</li>
<li>
<p>$J_{train}(\theta) \approx J_{cv}(\theta)$</p>
</li>
</ul>
</li>
<li>
<p>Variance ( Overfit, when d too large ):</p>
<ul>
<li>
<p>$J_{train}(\theta)$ will be low</p>
</li>
<li>
<p>$J_{cv}(\theta) \gg J_{train}(\theta)$</p>
</li>
</ul>
</li>
</ul>
<h4 id="regularization-and-bias--variance">Regularization and bias / variance</h4>
<ul>
<li>
<p>Linear Regression with Regularization</p>
<p>Model: $h_{\theta}(x) = \theta_0 + \theta_1x + \theta_2x^2 + \theta_3x^3 + \theta_4x^4$</p>
<p>Goal: $J(\theta) = \frac{1}{2m}\sum_{i = 1}^m(h_\theta(x^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2m}\sum_{j = 1}^n\theta_j^2$</p>
<p>$J_{train}(\theta) = \frac{1}{2m_{train}}\sum_{i = 1}^{m_{train}}(h_{\theta}(x_{train}^{(i)}) - y_{train}^{(i)})^2$</p>
<p>$J_{cv}(\theta) = \frac{1}{2m_{cv}}\sum_{i = 1}^{m_{cv}}(h_{\theta}(x_{cv}^{(i)}) - y_{cv}^{(i)})^2$</p>
<p>$J_{test}(\theta) = \frac{1}{2m_{test}}\sum_{i = 1}^{m_{test}}(h_{\theta}(x_{test}^{(i)}) - y_{test}^{(i)})^2$</p>
</li>
<li>
<p>Chossing the parameter $\lambda$</p>
<p>1). Try $\lambda = 0 \rarr minJ(\theta) \rarr \theta^{(1)} \rarr J_{cv}(\theta^{(1)})$</p>
<p>2). Try $\lambda = 0.01 \rarr minJ(\theta) \rarr \theta^{(2)} \rarr J_{cv}(\theta^{(2)})$</p>
<p>3). Try $\lambda = 0.02 \rarr minJ(\theta) \rarr \theta^{(3)} \rarr J_{cv}(\theta^{(3)})$</p>
<p>4). Try $\lambda = 0.04 \rarr minJ(\theta) \rarr \theta^{(4)} \rarr J_{cv}(\theta^{(4)})$</p>
<p>5). Try $\lambda = 0.08 \rarr minJ(\theta) \rarr \theta^{(5)} \rarr J_{cv}(\theta^{(5)})$</p>
<p>&hellip;</p>
<p>10). Try $\lambda = 10 \rarr minJ(\theta) \rarr \theta^{(10)} \rarr J_{cv}(\theta^{(10)})$</p>
<p>Pick $\theta^{(5)}$, Test Error: $J_{test}(\theta^{(5)})$</p>
</li>
<li>
<p>Draw the <strong>&ldquo;lambda - Error&rdquo; plot.</strong> In this Plot draw the $J_{train}(\theta), J_{cv/test}(\theta)$.</p>
</li>
<li>
<p>When $\lambda$ too large:</p>
<ul>
<li>
<p>$J_{train}(\theta)$ will be high</p>
</li>
<li>
<p>$J_{train}(\theta) \approx J_{cv}(\theta)$</p>
</li>
</ul>
</li>
<li>
<p>When $\lambda$ too small:</p>
<ul>
<li>
<p>$J_{train}(\theta)$ will be low</p>
</li>
<li>
<p>$J_{cv}(\theta) \gg J_{train}(\theta)$</p>
</li>
</ul>
</li>
<li>
<p><strong>The Situation is contrary to the Dimension d.</strong></p>
</li>
</ul>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/Bias_lambda.png"
        data-srcset="/Bias_lambda.png, /Bias_lambda.png 1.5x, /Bias_lambda.png 2x"
        data-sizes="auto"
        alt="/Bias_lambda.png"
        title="Bias_lambda" /></p>
<h4 id="learning-curves">Learning Curves</h4>
<ul>
<li>
<p>$J_{train}(\theta) = \frac{1}{2m_{train}}\sum_{i = 1}^{m_{train}}(h_{\theta}(x_{train}^{(i)}) - y_{train}^{(i)})^2$</p>
<p>$J_{cv}(\theta) = \frac{1}{2m_{cv}}\sum_{i = 1}^{m_{cv}}(h_{\theta}(x_{cv}^{(i)}) - y_{cv}^{(i)})^2$</p>
</li>
<li>
<p>Draw the <strong>&ldquo;m ( size of Training Set ) - Error&rdquo; plot.</strong> Reduce $m$ artificially.</p>
</li>
<li>
<p><strong>Training Set Error will increase when m increases, and the Cross Validation Error will decrease on the contrary.</strong></p>
</li>
<li>
<p>High Bias:</p>
</li>
</ul>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/high_bias.png"
        data-srcset="/high_bias.png, /high_bias.png 1.5x, /high_bias.png 2x"
        data-sizes="auto"
        alt="/high_bias.png"
        title="high_bias" /></p>
<ul>
<li>High Variance:</li>
</ul>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/high_variance.png"
        data-srcset="/high_variance.png, /high_variance.png 1.5x, /high_variance.png 2x"
        data-sizes="auto"
        alt="/high_variance.png"
        title="high_variance" /></p>
<p><strong>Watch the Gap!</strong> ( Between $J_{train}(\theta), J_{test}(\theta)$)</p>
<h4 id="how-to-deal-with-high-bias--high-variance">How to deal with high Bias &amp; high Variance?</h4>
<table>
<thead>
<tr>
<th style="text-align:center">high Bias</th>
<th style="text-align:center">high Variance</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Getting additional Features</td>
<td style="text-align:center">Getting more Training examples</td>
</tr>
<tr>
<td style="text-align:center">Try adding polynomial Features</td>
<td style="text-align:center">Try smaller sets of Features</td>
</tr>
<tr>
<td style="text-align:center">Try decreasing $\lambda$</td>
<td style="text-align:center">Try increasing $\lambda$</td>
</tr>
</tbody>
</table>
<h2 id="error-analysis">Error Analysis</h2>
<h3 id="recommand-approach">Recommand Approach</h3>
<ul>
<li>
<p>Start with a simple algorithm that you can implement quickly. Implement it and test it on Cross-Validation Set.</p>
</li>
<li>
<p>Plot learning curves to decide if more Data, more Features, etc.</p>
</li>
<li>
<p>Error Analysis: Manually examine the examples ( In <strong>Cross Validation Set</strong> ) that your algorithm made errors on. See if you  spot any systematic trend in what type of examples it is making errors on.</p>
</li>
</ul>
<h3 id="error-metrics-for-skewed-classes">Error metrics for skewed classes</h3>
<ul>
<li>
<p>Cancer classification example:</p>
<ul>
<li>
<p>Train logistic regression model $h_\theta(x). ( $y = 1$ if cancer )</p>
</li>
<li>
<p>Find you got $1%$ error on test set</p>
</li>
<li>
<p>Only $0.50%$ patients have cancer</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">function y = predictCancer(x)
  y = 0; // ignore x! Because 99.5% Patients don&#39;t have cancer!
return
</code></pre></td></tr></table>
</div>
</div></li>
</ul>
</li>
</ul>
<h4 id="precision--recall">Precision / Recall</h4>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">1</th>
<th style="text-align:center">0</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">True positiv</td>
<td style="text-align:center">False positiv</td>
</tr>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">False Negative</td>
<td style="text-align:center">True Negative</td>
</tr>
</tbody>
</table>
<ul>
<li>
<p>Precision( Of all patients where we predicted $y = 1$, what fraction actually has cancer? )</p>
<p>$\frac{True positives}{#predicted Positive} = \frac{True positives}{True Pos + False Pos}$</p>
</li>
<li>
<p>Recall( Of all patients that actually have cancer, what fraction did we corrently detect as having cancer? )</p>
<p>$\frac{True positives}{#actual Positives} = \frac{True Positives}{True Pos + False Neg}$</p>
</li>
</ul>
<h4 id="trading-off-precision-and-recall">Trading off precision and recall</h4>
<ul>
<li>
<p>Logistic Regression: $0 \leq h_\theta(x) \leq 1$
Predict 1 if $h_\theta(x) \geq 0.5$
Predict 0 if $h_\theta(x) \leq 0.5$</p>
</li>
<li>
<p>Suppose we want to predict $y = 1$ ( cancer ) only if very confident</p>
<p>$\rarr \text{Higher Precision, lower Recall}$</p>
</li>
<li>
<p>Suppose we want to avoid missing too many cases of cancer ( avoid false Negatives )</p>
<p>$\rarr \text{Higher Recall, lower Precision}$</p>
</li>
<li>
<p>More generally, predict 1 if $h_\theta(x) \geq \text{threshold}$</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/precision-recall.png"
        data-srcset="/precision-recall.png, /precision-recall.png 1.5x, /precision-recall.png 2x"
        data-sizes="auto"
        alt="/precision-recall.png"
        title="precision-recall" /></p>
</li>
</ul>
<h4 id="f-score">F Score</h4>
<ul>
<li>How to compare Precision / Recall numbers?</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">Precision (P)</th>
<th style="text-align:center">Recall (R)</th>
<th style="text-align:center">F Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Algorithm 1</td>
<td style="text-align:center">0.5</td>
<td style="text-align:center">0.4</td>
<td style="text-align:center">0.444</td>
</tr>
<tr>
<td style="text-align:center">Algorithm 2</td>
<td style="text-align:center">0.7</td>
<td style="text-align:center">0.1</td>
<td style="text-align:center">0.175</td>
</tr>
<tr>
<td style="text-align:center">Algorithm 3</td>
<td style="text-align:center">0.02</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0.0392</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>F Score:</strong> $2\frac{PR}{P + R}$</li>
</ul>
<h2 id="support-vector-machines">Support Vector Machines</h2>
<h3 id="optimization-objective">Optimization objective</h3>
<blockquote>
<p>Brief Review of Logistic Regression<br>
$h_\theta(x) = \frac{1}{1 + e^{-\theta^Tx}}$<br>
if $y = 1$, we want $h_\theta(x) \approx 1, \theta^Tx \gg 0$<br>
if $y = 0$, we want $h_\theta(x) \approx 0, \theta^Tx \ll 0$<br>
$J(\theta) = -\frac{1}{m}\sum_{i = 1}^{m}[y^{(i)}log(h_\theta(x^{(i)})) + (1 - y^{(i)})log(1 - h_\theta(x^{(i)}))] + \frac{\lambda}{2m}\sum_{j = 1}^{n}\theta_j^2$</p>
</blockquote>
<ul>
<li>
<p>$-log\frac{1}{1 + e^{-x}} \rarr Cost_1(\theta^Tx^{(i)})$</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/logsigmoid.png"
        data-srcset="/logsigmoid.png, /logsigmoid.png 1.5x, /logsigmoid.png 2x"
        data-sizes="auto"
        alt="/logsigmoid.png"
        title="logsigmoid" /></p>
</li>
<li>
<p>$-log(1 - \frac{1}{1 + e^{-x}}) \rarr Cost_0(\theta^Tx^{(i)})$</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/-logsigmoid.png"
        data-srcset="/-logsigmoid.png, /-logsigmoid.png 1.5x, /-logsigmoid.png 2x"
        data-sizes="auto"
        alt="/-logsigmoid.png"
        title="-logsigmoid" /></p>
</li>
</ul>
<h4 id="cost-function-2">Cost Function:</h4>
<ul>
<li>
<p>We use a new function to replace $-log\frac{1}{1 + e^{-x}} \text{and} -log(1 - \frac{1}{1 + e^{-x}})$. We name they as $Cost_1(\theta^Tx^{(i)})\text{and}Cost_0(\theta^Tx^{(i)})$. Then the Cost Function in SVM is:</p>
<p>$\frac{1}{m}\sum_{i = 1}^{m}[y^{(i)}Cost_1(\theta^Tx^{(i)}) + (1 - y^{(i)})Cost_0(\theta^Tx)] + \frac{\lambda}{2m}\sum_{j = 1}^{n}\theta_j^2$</p>
<p><strong>The following Plot is the Picture of 2 new Functions:</strong></p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/SVMcost.jpeg"
        data-srcset="/SVMcost.jpeg, /SVMcost.jpeg 1.5x, /SVMcost.jpeg 2x"
        data-sizes="auto"
        alt="/SVMcost.jpeg"
        title="SVM" /></p>
<p><strong>Our goal is to minimize the following Function:</strong></p>
<p>$min_\theta C\sum_{i = 1}^{m}[y^{(i)}Cost_1(\theta^Tx^{(i)}) + (1 - y^{(i)})Cost_0(\theta^Tx)] + \frac{1}{2}\sum_{j = 1}^{n}\theta_j^2$</p>
</li>
</ul>
<h4 id="svm-hypothesis">SVM Hypothesis:</h4>
<ul>
<li>$h_\theta(x) = \begin{cases}1, \theta^Tx \geq 0\\0, \theta^Tx &lt; 0\end{cases}$</li>
</ul>
<h3 id="large-margin-intuition">Large Margin Intuition</h3>
<ul>
<li>Take a look at the Plot of the new Cost Functions:
<ul>
<li>if $y = 1$, we want $\theta^Tx \geq 1$</li>
<li>if $y = 0$, we want $\theta^Tx \leq -1$</li>
</ul>
</li>
</ul>
<h4 id="svm-decision-boundary">SVM Decision Boundary</h4>
<ul>
<li>
<p>$min_\theta C\sum_{i = 1}^{m}[y^{(i)}Cost_1(\theta^Tx^{(i)}) + (1 - y^{(i)})Cost_0(\theta^Tx^{(i)})] + \frac{1}{2}\sum_{j = 1}^{n}\theta_j^2 ( C = \frac{1}{\lambda} )$</p>
<ul>
<li><strong>Large C: lower Bias, higher Variance</strong></li>
<li><strong>Small C: higher Bias, lower Variance</strong></li>
</ul>
</li>
<li>
<p>In &ldquo;Large Margin Boundary&rdquo; we know, we just have to optimize the following function:</p>
<p>$\frac{1}{2}\sum_{j = 1}^{n}\theta_j^2$ ( Because the front part can be 0. )</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/margin.png"
        data-srcset="/margin.png, /margin.png 1.5x, /margin.png 2x"
        data-sizes="auto"
        alt="/margin.png"
        title="margin" /></p>
</li>
</ul>
<h3 id="kernels">Kernels</h3>
<h4 id="kernel-function">Kernel Function</h4>
<ul>
<li>
<p>Given x,:</p>
<p>$f_i = similarity(x, l^{(i)}) = exp(-\frac{||x - l^{(2)}||^2}{2\sigma^2})$</p>
</li>
<li>
<p>Large $\sigma^2$: Features $f_i$ vary very smmothly: <strong>Higher Bias, lower Variance</strong></p>
</li>
<li>
<p>Small $\sigma^2$: Features $f_i$ vary very smmothly: <strong>lower Bias, higher Variance</strong></p>
</li>
</ul>
<h4 id="svm-with-kernels">SVM with Kernels</h4>
<ul>
<li>
<p>Where to get $l^{(1)}, l^{(2)}, l^{(3)} &hellip; ?$</p>
<ul>
<li>
<p>Given $(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), &hellip;, (x^{(m)}, y^{(m)})$</p>
</li>
<li>
<p>Choose $l^{(1)} = x^{(1)}, l^{(2)} = x^{(2)}, &hellip;, l^{(m)} = x^{(m)}$</p>
</li>
</ul>
</li>
<li>
<p>Given $x$:</p>
<p>$f_1 = similarity(x, l^{(1)})$</p>
<p>$f_2 = similarity(x, l^{(2)})$</p>
<p>&hellip;</p>
<p>$f = \begin{bmatrix}f_0\\f_1\\f_2\\&hellip;\\f_m\end{bmatrix}, f_0 = 1$</p>
</li>
<li>
<p>For Training example $(x^{(i)}, y^{(i)}):$</p>
<p>$f_1^{(i)} = similarity(x^{(i)}, l^{(1)})$</p>
<p>$f_2^{(i)} = similarity(x^{(i)}, l^{(2)})$</p>
<p>$f_3^{(i)} = similarity(x^{(i)}, l^{(3)})$</p>
<p>&hellip;</p>
<p>$f_m^{(i)} = similarity(x^{(i)}, l^{(m)})$</p>
<p>$f^{(x)} = \begin{bmatrix}f_0^{(i)}\\f_1^{(i)}\\&hellip;\\f_m^{(i)}\end{bmatrix}$</p>
</li>
<li>
<p>Predict $y = 1, \text{if } \theta^Tf \geq 0$</p>
</li>
<li>
<p>Training: $min_\theta C\sum_{i = 1}^{m}[y^{(i)}Cost_1(\theta^Tf^{(i)}) + (1 - y^{(i)})Cost_0(\theta^Tf^{(i)})] + \frac{1}{2}\sum_{j = 1}^{n}\theta_j^2$</p>
</li>
</ul>
<h3 id="logistic-regression-vs-svms">Logistic Regression vs. SVMs</h3>
<ul>
<li>
<p>n = number of Features, m = number of Training examples</p>
</li>
<li>
<p>if n is large ( relative to m ):</p>
<p>Use logistic Regression, or SVM without a kernel</p>
</li>
<li>
<p>if n is small, m is intermediate:</p>
<p>Use SVM with Gaussian Kernel</p>
</li>
<li>
<p>if n is small, m is large:</p>
<p>Create / Add more Features, then use logistic regression or SVM without a Kernel</p>
</li>
</ul>
<h2 id="unsupervised-learning">Unsupervised Learning</h2>
<h3 id="clustering">Clustering</h3>
<h4 id="k-mean-algorithm">K-Mean Algorithm</h4>
<ul>
<li>
<p>Input:</p>
<ul>
<li>
<p>K ( Number of clusters )</p>
</li>
<li>
<p>Training Set ${x^{(1)}, x^{(2)}, &hellip;, x^{(m)}}$</p>
</li>
<li>
<p>$x^{(i)} \isin \R^n$ ( Drop $x_0 = 1$ convention )</p>
</li>
</ul>
</li>
<li>
<p>Randomly initialize K cluster Centroids $\mu_1, \mu_2, &hellip;, \mu_k\isin\R^n$</p>
</li>
<li>
<p>Repeat:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">  // Cluster Assignment
  for i = 1 to m:
    c^(i) := index ( from 1 to K ) of Cluster Centroid closest to x^(i) // ||x^(i) - mu_k||
    
  // Move Centroid
  for k = 1 to K:
    mu_k := average ( mean ) of points asssigned to cluster k // A vector in R^n
</code></pre></td></tr></table>
</div>
</div><ul>
<li>
<p>In the code above</p>
<ul>
<li>
<p><em>$c^{(i)}$ means the index of cluster to which example $x^{(i)}$ is currently assigned.</em></p>
</li>
<li>
<p><em>$\mu_k\  \text{means the cluster centroid}\  k (\mu_k \isin\R^n)$</em></p>
</li>
<li>
<p><em>$\mu</em>{c^{(i)}}$ means the cluster centroid of cluster to which example $x^{(i)}$ has been assigned._</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="optimization-objective-of-k-mean-distortion-function">Optimization Objective of K-Mean: Distortion Function</h4>
<ul>
<li>$J(c^{(1)}, &hellip;, c^{(m)}, \mu_1, &hellip;, \mu_k) = \frac{1}{m}\sum_{i = 1}^m||x^{(i)} - \mu_{c^{(i)}}||^2$</li>
</ul>
<h4 id="initialize-k-means">Initialize K-Means</h4>
<ul>
<li>
<p>Randomly pick K training examples</p>
</li>
<li>
<p>Set $\mu_1, &hellip;, \mu_k$ equal to these K examples.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">  for i = 1 to 100{

    Randomly initialize K-means
    Run K-means, Get c^(1), ..., c^(m), mu^(1), ..., , mu^(K)
    Compute the distortion Function
  }

  Pick clustering that gave lowest distortion Function value.
</code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h4 id="choosing-the-number-of-clusters">Choosing the number of clusters</h4>
<ul>
<li>
<p>Alternative: Draw the &ldquo;K - Distortion Function Value&rdquo; plot. ( Elbow Method )</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/elbow.png"
        data-srcset="/elbow.png, /elbow.png 1.5x, /elbow.png 2x"
        data-sizes="auto"
        alt="/elbow.png"
        title="Elbow" /></p>
</li>
</ul>
<h2 id="dimentionality-reduction">Dimentionality Reduction</h2>
<h3 id="pac-principal-component-analysis">PAC: Principal Component Analysis</h3>
<ul>
<li>
<p>Reduce from 2-dimension to 1-dimension: Find a direction ( a vector $\mu^{(1)} \isin \R^n$ ) onto which to project the data so as to minimize the projection error.</p>
</li>
<li>
<p>Reduce from n-dimension to k-dimension: Find k vectors ( vectors $\mu^{(1)}, &hellip;, \mu^{(k)}$ ) onto which to project the data, so as to minimize the projection error.</p>
</li>
<li>
<p>Reduce memory / disk needed to store data</p>
</li>
<li>
<p>Speed up learning algorithm</p>
</li>
<li>
<p><strong>Bad use of PCA: To prevent overfitting. Instead, use regularization.</strong></p>
</li>
<li>
<p><strong>PCA is not Linear Regression!</strong></p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/PCANlinear.png"
        data-srcset="/PCANlinear.png, /PCANlinear.png 1.5x, /PCANlinear.png 2x"
        data-sizes="auto"
        alt="/PCANlinear.png"
        title="PCANlinear" /></p>
</li>
</ul>
<h3 id="data-processing">Data Processing</h3>
<ul>
<li>
<p>Training Set: $x^{(1)}, x^{(2)}, &hellip;, x^{(m)}$</p>
</li>
<li>
<p>Preprocessing ( Feature Scaling / Mean Normalization )</p>
<p>$\mu_j = \frac{1}{m}\sum_{i = 1}^mx_j^{(i)}$</p>
<p>Replace each $x_j^{(i)}$ with $x_j - \mu_j$</p>
<p>If different features have different scales, scale Features to have comparable range of values $x_j \larr \frac{x_j^{(i)} - \mu_j}{s_j}$</p>
</li>
</ul>
<h3 id="pca-algorithm">PCA algorithm</h3>
<ul>
<li>
<p>Reduce data from n-dimensions to k-dimensions</p>
</li>
<li>
<p>Compute &ldquo;convariance matrix&rdquo;</p>
<p>$\Sigma = \frac{1}{m}\sum_{i = 1}^{m}(x^{(i)})(x^{(i)})^T, \Sigma \isin \R^{n\times n}$</p>
</li>
<li>
<p>Compute the &ldquo;eigenvectors&rdquo; of $\Sigma$:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">  [U, S, V] = svd( Sigma ) // Singular Value Decomposition;
  Uredece = U(:, 1:k);
  z = Ureduce&#39; * x;  
</code></pre></td></tr></table>
</div>
</div><p>From the formular above we get:</p>
<p>$U = \begin{bmatrix}|&amp;|&amp;&hellip;&amp;|\\u^{(1)}&amp;u^{(2)}&amp;&hellip;&amp;u^{(n)}\\|&amp;|&amp;&hellip;&amp;|\end{bmatrix} \isin \R^{n\times n}$</p>
<p>Then we use the first k columns in the Matrix:</p>
<p>$z^{(i)} = U_{reduce}^Tx^{(i)} = \begin{bmatrix}|&amp;|&amp;&hellip;&amp;|\\u^{(1)}&amp;u^{(2)}&amp;&hellip;&amp;u^{(k)}\\|&amp;|&amp;&hellip;&amp;|\end{bmatrix}^Tx^{(i)} = \begin{bmatrix}-&amp;(u^{(1)})^T&amp;-\\&hellip;&amp;&hellip;&amp;&hellip;\\-&amp;(u^{(k)})^T&amp;-\end{bmatrix}x^{(i)} \isin \R^{k\times 1}$</p>
</li>
</ul>
<h3 id="choosing-k--number-of-principle-components-">Choosing k ( number of principle components )</h3>
<ul>
<li>
<p>Average squaied projection error: $\frac{1}{m}\sum_{i = 1}^m||x^{(i)} - x_{approx}^{(i)}||^2$</p>
</li>
<li>
<p>Total variation in the data: $\frac{1}{m}\sum_{i = 1}^{m}||x^{(i)}||^2$</p>
</li>
<li>
<p>Typically, choose k to be smallest value so that:</p>
<p>$\frac{\frac{1}{m}\sum_{i = 1}^m||x^{(i)} - x^{(i)}_{approx}||^2}{\frac{1}{m}\sum_{i = 1}^{m}||x^{(i)}||^2} \leq 0.01$</p>
<p>&ldquo;99% of variances is retained&rdquo;</p>
</li>
<li>
<p><strong>Algorithm:</strong></p>
<ul>
<li>
<p>Try PCA with k = 1</p>
</li>
<li>
<p>Compute $U_{reduce}, z^{(1)}, z^{(2)}, &hellip;, z^{(m)}, x_{approx}^{(1)}, &hellip;, x^{(m)}_{approx}$</p>
</li>
<li>
<p>Check if $\frac{\frac{1}{m}\sum_{i = 1}^m||x^{(i)} - x^{(i)}_{approx}||^2}{\frac{1}{m}\sum_{i = 1}^{m}||x^{(i)}||^2} \leq 0.01$</p>
<p><code>[U, S, V] = svd( Sigma )</code></p>
<p>For given k, $\frac{\frac{1}{m}\sum_{i = 1}^m||x^{(i)} - x^{(i)}_{approx}||^2}{\frac{1}{m}\sum_{i = 1}^{m}||x^{(i)}||^2} = 1 - \frac{\sum_{i = 1}^ks_{ii}}{\sum_{i = 1}^{n}s_{ii}} \rarr \frac{\sum_{i = 1}^ks_{ii}}{\sum_{i = 1}^{n}s_{ii}} \geq 0.99$</p>
</li>
<li>
<p>Pick k, that satisfies $\frac{\sum_{i = 1}^ks_{ii}}{\sum_{i = 1}^{n}s_{ii}} \geq 0.99$</p>
</li>
</ul>
</li>
</ul>
<h3 id="reconstruction-from-compressed-representation">Reconstruction from compressed representation</h3>
<ul>
<li>$X_{approx} = U_{reduce} \times z$</li>
</ul>
<h3 id="design-of-ml-system-with-pca">Design of ML system with PCA</h3>
<ul>
<li>
<p>Get Training Set</p>
</li>
<li>
<p>Run PCA to reduce $x^{(i)}$ in dimension to get $z^{(i)}$</p>
</li>
<li>
<p>Train logistic regression on ${(z^{(1)}, y^{(1)}), &hellip;, (z^{(m)}, y^{(m)})}$</p>
</li>
<li>
<p>Test on test set: Map $x_{test}^{(i)}\ \text{to}\ z_{test}^{(i)}$. Run $h_\theta(z)$ on ${(z_{test}^{(1)}, y_{test}^{(1)}), &hellip;, (z_{test}^{(m)}, y_{test}^{(m)})}$</p>
</li>
<li>
<p>Before implementing PCA, first try running whatever you want to do with original / raw data $x^{(i)}$. Only if that doesn&rsquo;t do what you want, then implement PCA and consider using $z^{(i)}$</p>
</li>
</ul>
<h2 id="anomoly-detection">Anomoly Detection</h2>
<h3 id="example-fraud-detection">Example: Fraud Detection</h3>
<ul>
<li>
<p>$x^{(i)}$ = features of user i&rsquo;s activities</p>
</li>
<li>
<p>Model $p(x)$ from data</p>
</li>
<li>
<p>Identify unusual users by checking which have $p(x) &lt; \epsilon$</p>
</li>
</ul>
<h3 id="gaussian-distribution">Gaussian Distribution</h3>
<ul>
<li>
<p>Say $x \isin \R$. If x is a disatributed Gaussian with mean $\mu$, variance $\sigma^2$.</p>
</li>
<li>
<p>$p(x; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi}\sigma}exp^{(-\frac{(x - \mu)^2}{2\sigma^2})}$. Here $\sigma$ is the standard deviation.</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/Gaussian.svg"
        data-srcset="/Gaussian.svg, /Gaussian.svg 1.5x, /Gaussian.svg 2x"
        data-sizes="auto"
        alt="/Gaussian.svg"
        title="Gaussian" /></p>
</li>
<li>
<p>As for &ldquo;Parameter estimation&rdquo;&hellip;</p>
<ul>
<li>$\mu = \frac{1}{m}\sum_{i = 1}^mx^{(i)}$</li>
<li>$\sigma^2 = \frac{1}{m}\sum_{i = 1}^m(x^{(i)} - \mu)^2$</li>
</ul>
</li>
</ul>
<h3 id="algorithm">Algorithm</h3>
<ul>
<li>
<p>Training Set ${x^{(1)}, &hellip;, x^{(m)}}$</p>
</li>
<li>
<p>Each Example is $x \isin \R^n$</p>
</li>
</ul>
<ol>
<li>
<p>Chosse features $x_i$ that you think might be indicative of anomalous examples.</p>
</li>
<li>
<p>Fit parameters $\mu_1, &hellip;, \mu_n, \sigma_1^2, &hellip;, \sigma_n^2$</p>
<p>$\mu = \frac{1}{m}\sum_{i = 1}^mx^{(i)}$</p>
<p>$\sigma^2 = \frac{1}{m}\sum_{i = 1}^m(x^{(i)} - \mu)^2$</p>
</li>
<li>
<p>Given new example x, compute $p(x)$</p>
<p>$p(x) = \prod_{j = 1}^np(x_j;\mu_j, \sigma_j^2) = \prod_{j = 1}^n\frac{1}{\sqrt{2\pi}\sigma}exp^{(-\frac{(x - \mu)^2}{2\sigma^2})}$</p>
</li>
</ol>
<p>Anomaly if $p(x) &lt; \epsilon$</p>
<h4 id="evaluation">Evaluation</h4>
<ul>
<li>
<p>Fit model $p(x)$ on training set ${x^{(1)}, &hellip;, x^{(m)}}$</p>
</li>
<li>
<p>On a Cross Validation / Test example, predict</p>
<p>$y = \begin{cases}1,\ p(x) &lt; \epsilon\ \text{(anomaly)}\\0,\ p(x) \geq \epsilon\ \text{(normal)}\end{cases}$</p>
</li>
<li>
<p>Possible evaluation metrics:</p>
<ul>
<li>
<p>True positive, false positive, false negative, true negative</p>
</li>
<li>
<p>Precision / Recall</p>
</li>
<li>
<p>F-Score</p>
</li>
</ul>
<p>Can also use the Cross Validation Set to choose parameter $\epsilon$</p>
</li>
</ul>
<h3 id="anomoly-detection-vs-supervised-learning">Anomoly Detection vs. Supervised Learning</h3>
<table>
<thead>
<tr>
<th style="text-align:center">Anomoly Detection</th>
<th style="text-align:center">Supervised Learning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Very small number of positiv examples but large number of negative examples</td>
<td style="text-align:center">Large number of positive and negative  examples</td>
</tr>
<tr>
<td style="text-align:center">Many different types of anomalies. Hard for any algorithm to learn from positive examples what the anomalies looks like</td>
<td style="text-align:center">Enough positive examples for algorithm to get a sense of what positive examples are like</td>
</tr>
<tr>
<td style="text-align:center">Future anomalies may look nothing like any of the anomalous examples we&rsquo;ve seen so far</td>
<td style="text-align:center">Future positive examples likely to be similar to ones in training set</td>
</tr>
</tbody>
</table>
<h3 id="error-analysis-for-anomaly-detection">Error Analysis for anomaly detection</h3>
<ul>
<li>
<p>Want $p(x)$ large for normal examples x</p>
<p>$p(x)$ small for anomalous examples x</p>
</li>
<li>
<p>Most commen Problem:</p>
<p>$p(x)$ is comparable ( say, both large ) for normal and anomalous examples.</p>
</li>
</ul>
<h3 id="multivariance-gaussian-distribution">Multivariance Gaussian Distribution</h3>
<ul>
<li>
<p>$x \isin \R^n$, don&rsquo;t model $p(x_1), p(x_2)$, &hellip;, etc. separately. Instead, model $p(x)$ all in one go.</p>
</li>
<li>
<p>Parameters: $\mu\isin\R^n, \Sigma \isin \R^{n\times n}$, the &ldquo;convariance matrix&rdquo;</p>
</li>
<li>
<p>$p(x; \mu, \Sigma) = \frac{1}{(2\pi)^{(\frac{n}{2})}|\Sigma|^\frac{1}{2}}exp(=\frac{1}{2}(x - \mu)^T\Sigma^{-1}(x - \mu))$</p>
</li>
</ul>
<h2 id="recommander-systems">Recommander Systems</h2>
<h3 id="problem-formulation">Problem Formulation</h3>
<h4 id="example-predicting-movie-ratings">Example: Predicting movie ratings</h4>
<table>
<thead>
<tr>
<th style="text-align:center">Movie</th>
<th style="text-align:center">Alice(1)</th>
<th style="text-align:center">Bob(2)</th>
<th style="text-align:center">Carol(3)</th>
<th style="text-align:center">Dave(4)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Love at last</td>
<td style="text-align:center">5</td>
<td style="text-align:center">5</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">Romance forever</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">Cute Puppies of love</td>
<td style="text-align:center">?</td>
<td style="text-align:center">4</td>
<td style="text-align:center">0</td>
<td style="text-align:center">?</td>
</tr>
<tr>
<td style="text-align:center">Nonstop car cashes</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">4</td>
</tr>
<tr>
<td style="text-align:center">Swords vs. karate</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
</tr>
</tbody>
</table>
<ul>
<li>
<p>$n_u$: no.users</p>
<p>$n_m$: no.movies</p>
<p>$r(i, j) = 1$: if user $j$ has rated movie $i$</p>
<p>$y^{(i,j)} \isin [0,5]$: rating given by user $j$ to movie $i$ ( defined only if $r(i, j) = 1$ )</p>
</li>
</ul>
<h3 id="contnet-based-recommendations">Contnet-based recommendations</h3>
<table>
<thead>
<tr>
<th style="text-align:center">Movie</th>
<th style="text-align:center">Alice(1)</th>
<th style="text-align:center">Bob(2)</th>
<th style="text-align:center">Carol(3)</th>
<th style="text-align:center">Dave(4)</th>
<th style="text-align:center">$x_1$(romance)</th>
<th style="text-align:center">$x_2$(action)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Love at last</td>
<td style="text-align:center">5</td>
<td style="text-align:center">5</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0.9</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">Romance forever</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">0.01</td>
</tr>
<tr>
<td style="text-align:center">Cute Puppies of love</td>
<td style="text-align:center">?</td>
<td style="text-align:center">4</td>
<td style="text-align:center">0</td>
<td style="text-align:center">?</td>
<td style="text-align:center">0.99</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">Nonstop car cashes</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">4</td>
<td style="text-align:center">0.1</td>
<td style="text-align:center">1.0</td>
</tr>
<tr>
<td style="text-align:center">Swords vs. karate</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0.9</td>
</tr>
</tbody>
</table>
<ul>
<li>
<p>For each user j, learn a parameter $\theta^{(j)} \isin \R^3$. Predict user j as rating movie i with $(\theta^{(j)})^Tx^{(i)}$ stars. ( $x_0 = 1$ )</p>
</li>
<li>
<p>$\theta^{(j)} =$ parameter vector for user $j$</p>
<p>$x^{(i)} =$ feature vector for movie $i$</p>
<p>For user j, movie i, predicting rating: $(\theta^{(j)})^Tx^{(i)}$</p>
<p>$m^{(j)} =$ no.of movies rated by user $j$</p>
</li>
<li>
<p>To learn $\theta^{(j)}$:</p>
<p>$min_{\theta^{(j)}}\frac{1}{2m^{(j)}}\sum_{i:r(i, j) = 1}((\theta^{(j)})^T(x^{(i)} - y^{(i, j)})^2 + \frac{\lambda}{2m^{(j)}}\sum_{k = 1}^{n}(\theta_k^{(j)})^2$</p>
<p>$\rarr min_{\theta^{(j)}}\frac{1}{2}\sum_{i:r(i, j) = 1}((\theta^{(j)})^T(x^{(i)} - y^{(i, j)})^2 + \frac{\lambda}{2}\sum_{k = 1}^{n}(\theta_k^{(j)})^2$</p>
</li>
<li>
<p>To learn $\theta^{(1)}, \theta^{(2)}, &hellip;, \theta^{(n_u)}$:</p>
<p>$min_{\theta^{(1)}, \theta^{(2)}, &hellip;, \theta^{(n_u)}}\frac{1}{2}\sum_{j = 1}^{n_u}\sum_{i:r(i, j) = 1}((\theta^{(j)})^T(x^{(i)} - y^{(i, j)})^2 + \frac{\lambda}{2}\sum_{j = 1}^{n_u}\sum_{k = 1}^{n}(\theta_k^{(j)})^2$</p>
</li>
</ul>
<h4 id="optimization-algorithm">Optimization Algorithm</h4>
<ul>
<li>
<p>$min_{\theta^{(1)}, \theta^{(2)}, &hellip;, \theta^{(n_u)}}\frac{1}{2}\sum_{j = 1}^{n_u}\sum_{i:r(i, j) = 1}((\theta^{(j)})^T(x^{(i)} - y^{(i, j)})^2 + \frac{\lambda}{2}\sum_{j = 1}^{n_u}\sum_{k = 1}^{n}(\theta_k^{(j)})^2$</p>
</li>
<li>
<p>Gradient Descent update:</p>
<p>$\theta_k^{(j)} \coloneqq \theta_k^{(j)} - \alpha\sum_{i:r(i, j) = 1}((\theta^{(j)})^Tx^{(i)} - y^{(i, j)})x_k^{(i)}$ ( for $k = 0$ )</p>
<p>$\theta_k^{(j)} \coloneqq \theta_k^{(j)} - \alpha(\sum_{i:r(i, j) = 1}((\theta^{(j)})^Tx^{(i)} - y^{(i, j)})x_k^{(i)} + \lambda\theta_k^{(j)})$ ( for $k \not = 0$ )</p>
</li>
</ul>
<h3 id="collaborative-filtering">Collaborative Filtering</h3>
<h4 id="optimization-algorithm-1">Optimization Algorithm</h4>
<ul>
<li>
<p>Given $\theta^{(1)}, \theta^{(2)}, &hellip;, \theta^{(n_u)}$, to learn $x^{(i)}$:</p>
<p>$min_{x^{(i)}}\frac{1}{2}\sum_{j = 1}^{n_u}\sum_{i:r(i, j) = 1}((\theta^{(j)})^T(x^{(i)} - y^{(i, j)})^2 + \frac{\lambda}{2}\sum_{k = 1}^{n}(x_k^{(i)})^2$</p>
</li>
<li>
<p>Given $\theta^{(1)}, \theta^{(2)}, &hellip;, \theta^{(n_u)}$, to learn $x^{(1)}, &hellip;, x^{(n_u)}$:</p>
<p>$min_{x^{(1)}, x^{(2)}, &hellip;, x^{(n_m)}}\frac{1}{2}\sum_{i = 1}^{n_m}\sum_{i:r(i, j) = 1}((\theta^{(j)})^T(x^{(i)} - y^{(i, j)})^2 + \frac{\lambda}{2}\sum_{i = 1}^{n_m}\sum_{k = 1}^{n}(x_k^{(i)})^2$</p>
</li>
</ul>
<h4 id="collaborative-filtering-1">Collaborative Filtering</h4>
<ul>
<li>
<p>Given $x^{(1)}, x^{(2)}, &hellip;, x^{(n_m)}$ (and movie ratings), can estimate $\theta^{(1)}, \theta^{(2)}, &hellip;, \theta^{(n_u)}$</p>
</li>
<li>
<p>Given $\theta^{(1)}, \theta^{(2)}, &hellip;, \theta^{(n_u)}$, can estimate $x^{(1)}, x^{(2)}, &hellip;, x^{(n_m)}$</p>
</li>
<li>
<p>Guess $\theta \rarr x \rarr \theta \rarr x \rarr \theta \rarr x \rarr &hellip;$</p>
</li>
</ul>
<h4 id="collaborative-filtering-algorithm">Collaborative Filtering Algorithm</h4>
<ul>
<li>
<p>Collaborative flitering optimization objective</p>
<ul>
<li>
<p>Given $x^{(1)}, x^{(2)}, &hellip;, x^{(n_m)}$, estimate $\theta^{(1)}, \theta^{(2)}, &hellip;, \theta^{(n_u)}$</p>
<p>$min_{\theta^{(1)}, \theta^{(2)}, &hellip;, \theta^{(n_u)}}\frac{1}{2}\sum_{j = 1}^{n_u}\sum_{i:r(i, j) = 1}((\theta^{(j)})^T(x^{(i)} - y^{(i, j)})^2 + \frac{\lambda}{2}\sum_{j = 1}^{n_u}\sum_{k = 1}^{n}(\theta_k^{(j)})^2$</p>
</li>
<li>
<p>Given $\theta^{(1)}, \theta^{(2)}, &hellip;, \theta^{(n_u)}$, estimate $x^{(1)}, x^{(2)}, &hellip;, x^{(n_m)}$</p>
<p>$min_{x^{(1)}, x^{(2)}, &hellip;, x^{(n_m)}}\frac{1}{2}\sum_{i = 1}^{n_m}\sum_{i:r(i, j) = 1}((\theta^{(j)})^T(x^{(i)} - y^{(i, j)})^2 + \frac{\lambda}{2}\sum_{i = 1}^{n_m}\sum_{k = 1}^{n}(x_k^{(i)})^2$</p>
</li>
<li>
<p>$min_{(x^{(1)}, x^{(2)}, &hellip;, x^{(n_m)}, \theta^{(1)}, \theta^{(2)}, &hellip;, \theta^{(n_u)})}\frac{1}{2}\sum_{(i, j):r(i, j) = 1}((\theta^{(j)})^T(x^{(i)} - y^{(i, j)})^2 + \frac{\lambda}{2}\sum_{i = 1}^{n_m}\sum_{k = 1}^{n}(x_k^{(i)})^2 + \frac{\lambda}{2}\sum_{j = 1}^{n_u}\sum_{k = 1}^{n}(\theta_k^{(j)})^2$</p>
</li>
</ul>
</li>
<li>
<p>Collaborative flitering Algorithm</p>
</li>
</ul>
<ol>
<li>
<p>Initialize $x^{(1)}, x^{(2)}, &hellip;, x^{(n_m)}, \theta^{(1)}, \theta^{(2)}, &hellip;, \theta^{(n_u)}$ to small random values</p>
</li>
<li>
<p>Minimize $J(x^{(1)}, x^{(2)}, &hellip;, x^{(n_m)}, \theta^{(1)}, \theta^{(2)}, &hellip;, \theta^{(n_u)})$ using gradient descent ( or advanced optimization algorithm ). E.g. For every $j = 1, &hellip;, n_u, i = 1, &hellip;. n_m:$</p>
<p>$x_k^{(i)} \coloneqq x_k^{(i)} - \alpha(\sum_{j:r(i, j) = 1}((\theta^{(j)})^T(x^{(i)} - y^{(i, j)}))\theta_k^{(j)} + \lambda x_k^{(i)})$</p>
<p>$\theta_k^{(j)}\coloneqq\theta_k^{(j)} - \alpha(\sum_{i:r(i, j) = 1}((\theta^{(j)})^T(x^{(i)} - y^{(i, j)}))\theta_k^{(j)} + \lambda \theta_k^{(j)})$</p>
</li>
<li>
<p>For a user with parameters $\theta$ and a movie with (learned) features $x$, predict a star rating $\theta^Tx$.</p>
</li>
</ol>
<h4 id="vectorization-low-rank-matrix-factorization">Vectorization: Low rank matrix factorization</h4>
<table>
<thead>
<tr>
<th style="text-align:center">Movie</th>
<th style="text-align:center">Alice(1)</th>
<th style="text-align:center">Bob(2)</th>
<th style="text-align:center">Carol(3)</th>
<th style="text-align:center">Dave(4)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Love at last</td>
<td style="text-align:center">5</td>
<td style="text-align:center">5</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">Romance forever</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">Cute Puppies of love</td>
<td style="text-align:center">?</td>
<td style="text-align:center">4</td>
<td style="text-align:center">0</td>
<td style="text-align:center">?</td>
</tr>
<tr>
<td style="text-align:center">Nonstop car cashes</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">4</td>
</tr>
<tr>
<td style="text-align:center">Swords vs. karate</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
</tr>
</tbody>
</table>
<p>$Y = \begin{bmatrix}5&amp;5&amp;0&amp;0&amp;?\\5&amp;?&amp;?&amp;0&amp;?\\?&amp;4&amp;0&amp;?&amp;?\\0&amp;0&amp;5&amp;4&amp;?\\0&amp;0&amp;5&amp;0&amp;?\end{bmatrix}$</p>
<p>Predicted ratings ($X\Theta^T, (i, j) = (\theta^{(j)})^T(x^{(i)})$):</p>
<p>$\begin{bmatrix}(\theta^{(1)})^T(x^{(1)})&amp;(\theta^{(2)})^T(x^{(1)})&amp;&hellip;&amp;(\theta^{(n_u)})^T(x^{(1)})\\(\theta^{(1)})^T(x^{(2)})&amp;(\theta^{(2)})^T(x^{(2)})&amp;&hellip;&amp;(\theta^{(n_u)})^T(x^{(2)})\\&hellip;&amp;&hellip;&amp;&hellip;&amp;&hellip;&amp;\\(\theta^{(1)})^T(x^{(n_m)})&amp;(\theta^{(2)})^T(x^{(n_m)})&amp;&hellip;&amp;(\theta^{(n_u)})^T(x^{(n_m)})\end{bmatrix}$</p>
<p>$X = \begin{bmatrix}-&amp;(x^{(1)})^T&amp;-\\-&amp;(x^{(2)})^T&amp;-\\&hellip;&amp;&hellip;&amp;&hellip;\\-&amp;(x^{(n_m)})^T&amp;-\\\end{bmatrix}$
$\Theta = \begin{bmatrix}-&amp;(\theta^{(1)})^T&amp;-\\-&amp;(\theta^{(2)})^T&amp;-\\&hellip;&amp;&hellip;&amp;&hellip;\\-&amp;(\theta^{(n_m)})^T&amp;-\\\end{bmatrix}$</p>
<ul>
<li>
<p>Finding related movies</p>
<p>For each product $i$, we learn a feature vector $x^{(i)}\isin \R^n$.</p>
<p>How to find movies $j$ related to movie $i$?</p>
<p>Find movies with the smallest $||x^{(i)} - x^{(j)}||$</p>
</li>
</ul>
<h4 id="mean-normalization">Mean Normalization</h4>
<table>
<thead>
<tr>
<th style="text-align:center">Movie</th>
<th style="text-align:center">Alice(1)</th>
<th style="text-align:center">Bob(2)</th>
<th style="text-align:center">Carol(3)</th>
<th style="text-align:center">Dave(4)</th>
<th style="text-align:center">Eve(5)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Love at last</td>
<td style="text-align:center">5</td>
<td style="text-align:center">5</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">?</td>
</tr>
<tr>
<td style="text-align:center">Romance forever</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
<td style="text-align:center">0</td>
<td style="text-align:center">?</td>
</tr>
<tr>
<td style="text-align:center">Cute Puppies of love</td>
<td style="text-align:center">?</td>
<td style="text-align:center">4</td>
<td style="text-align:center">0</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
</tr>
<tr>
<td style="text-align:center">Nonstop car cashes</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">4</td>
<td style="text-align:center">?</td>
</tr>
<tr>
<td style="text-align:center">Swords vs. karate</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
</tr>
</tbody>
</table>
<p>$Y = \begin{bmatrix}5&amp;5&amp;0&amp;0&amp;?\\5&amp;?&amp;?&amp;0&amp;?\\?&amp;4&amp;0&amp;?&amp;?\\0&amp;0&amp;5&amp;4&amp;?\\0&amp;0&amp;5&amp;0&amp;?\end{bmatrix}$
$\mu = \begin{bmatrix}2.5\\2.5\\2\\2.25\\1.25\end{bmatrix}\rarr Y = \begin{bmatrix}2.5&amp;2.5&amp;-2.5&amp;-2.5&amp;?\\2.5&amp;?&amp;?&amp;-2.5&amp;?\\?&amp;2&amp;-2&amp;?&amp;?\\-2.25&amp;-2.25&amp;2.75&amp;1.75&amp;?\\-1.25&amp;-1.25&amp;3.75&amp;-1.25&amp;?\end{bmatrix}$</p>
<p>$min_{(x^{(1)}, x^{(2)}, &hellip;, x^{(n_m)}, \theta^{(1)}, \theta^{(2)}, &hellip;, \theta^{(n_u)})}\frac{1}{2}\sum_{(i, j):r(i, j) = 1}((\theta^{(j)})^T(x^{(i)} - y^{(i, j)})^2 + \frac{\lambda}{2}\sum_{i = 1}^{n_m}\sum_{k = 1}^{n}(x_k^{(i)})^2 + \frac{\lambda}{2}\sum_{j = 1}^{n_u}\sum_{k = 1}^{n}(\theta_k^{(j)})^2$</p>
<p>For user j, on movie i predict:</p>
<p>$\rarr (\theta^{(j)})^T(x^{(i)}) + \mu_i$</p>
<p>User Eve:</p>
<p>$\theta^{(5)} = \begin{bmatrix}0\\0\end{bmatrix}\rarr (\theta^{(5)})^T(x^{(i)}) + \mu_i$</p>
<h2 id="learning-with-large-datasets">Learning with large datasets</h2>
<h3 id="stochastic-gradient-descent">Stochastic gradient descent</h3>
<ul>
<li>
<p>$Cost(\theta, (x^{(i)}, y^{(i)}) = \frac{1}{2}(h_\theta(x^{(i)}) - y^{(i)})^2$</p>
<p>$J_{train}(\theta) = \frac{1}{m}\sum_{i = 1}^mcost(\theta, (x^{(i)}, y^{(i)})$</p>
</li>
<li>
<p>Randomly shuffle training examples, repeat $\theta_j\coloneqq\theta_j - \alpha(h_\theta(x^{(i)}) - y^{(i)})x^{(i)}$</p>
</li>
</ul>
<h3 id="mini-batch-gradient-descent">Mini-batch gradient descent</h3>
<ul>
<li>
<p>Batch gradient descent: Use all m examples in each iteration</p>
</li>
<li>
<p>Stochastic gradient descent: Use 1 example in each iteration</p>
</li>
<li>
<p>Mini-batch gradient descent: Use b examples in each iteration</p>
</li>
</ul>
<h3 id="convergence">Convergence</h3>
<ul>
<li>
<p>Batch gradient descent</p>
<p>Plot $J_{train}(\theta)$ as a function of the number of iterations of gradient descent.</p>
</li>
<li>
<p>Stochastic gradient descent:</p>
<p>During learning, compute $cost(\theta, (x^{(i)}, y^{(i)}))$ before updating $\theta$ using $(x^{(i)}, y^{(i)})$.</p>
<p>Every 1000 iterations(say), plot $cost(\theta, (x^{(i)}, y^{(i)}))$ averaged over the last 1000 examples processed by algorithm.</p>
</li>
</ul>
<h2 id="at-the-end">At the End&hellip;</h2>
<p><strong>Thank you, Andrew Ng!</strong></p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2020-12-30&nbsp;<a class="git-hash" href="https://github.com/dillonzq/LoveIt/commit/ea67c60d9104c66164901db99d59e11a5c24928d" target="_blank" title="commit by DaiJiabin(kechou.dai@gmail.com) ea67c60d9104c66164901db99d59e11a5c24928d: Processing Wed 30 Dec 2020 08:45:51 AM CET">
                                    <i class="fas fa-hashtag fa-fw"></i>ea67c60</a></span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/machine-learning/index.md" target="_blank">Read Markdown</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Twitter" data-sharer="twitter" data-url="https://daijiabin.github.io/machine-learning/" data-title="Machine Learning" data-hashtags="AI"><i class="fab fa-twitter fa-fw"></i></a><a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="https://daijiabin.github.io/machine-learning/" data-hashtag="AI"><i class="fab fa-facebook-square fa-fw"></i></a><a href="javascript:void(0);" title="Share on Linkedin" data-sharer="linkedin" data-url="https://daijiabin.github.io/machine-learning/"><i class="fab fa-linkedin fa-fw"></i></a><a href="javascript:void(0);" title="Share on WhatsApp" data-sharer="whatsapp" data-url="https://daijiabin.github.io/machine-learning/" data-title="Machine Learning" data-web><i class="fab fa-whatsapp fa-fw"></i></a><a href="javascript:void(0);" title="Share on 微博" data-sharer="weibo" data-url="https://daijiabin.github.io/machine-learning/" data-title="Machine Learning"><i class="fab fa-weibo fa-fw"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw"></i>&nbsp;<a href="/tags/ai/">AI</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/nodejs/" class="prev" rel="prev" title="NodeJS"><i class="fas fa-angle-left fa-fw"></i>NodeJS</a></div>
</div>
</article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">
                </div><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2020</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="https://www.github.com/DaiJiabin" target="_blank">Dai Jiabin</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/smooth-scroll@16.1.3/dist/smooth-scroll.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.37.1/dist/autocomplete.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/algoliasearch@4.2.0/dist/algoliasearch-lite.umd.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.2.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.4.0/sharer.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/typeit@7.0.4/dist/typeit.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":10},"comment":{},"data":{"id-1":"BE AWESOME!","id-2":"BE AWESOME!"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"PASDMWALPK","algoliaIndex":"index.en","algoliaSearchKey":"b42948e51daaa93df92381c8e2ac0f93","highlightTag":"em","maxResultLength":10,"noResultsFound":"No results found","snippetLength":30,"type":"algolia"},"typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"id-1":["id-1"],"id-2":["id-2"]},"duration":-1,"speed":100}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
