[{"categories":null,"content":"This is a Test File… But I’m sure I’ll write it :) . 20.12.2020 一语盖之，“魔幻”。 新冠疫情八千公里开外肆虐作序，学业和生活受到愈来愈大的影响， 这都是在搭上出国那趟航班前想都不敢想的事情。在那之前所有的憧憬，哪怕是认识几个对中国文化感兴趣的外国同学，都瞬间化作了泡影。 但似乎也没什么大不了的。只是有时候认真的回想，不带口罩的日子，还有临别前的握手、拥抱，是很遥远的日子了。去年的这个时候，疫情尚未波及德国，还有空气中满溢着Gluehwein香甜气息的圣诞市场。活在当下真的是一个很重要的教训……去年没有好好体验这边的圣诞节，今年街道又冷清如同死城——只能等明年了。 只希望疫情能尽快过去。 “愿世界和平”从调侃变成了很正经的一句话。有种说不上来的滋味。 ","date":"2020-12-19","objectID":"/zh-cn/diary/see-ya-2020/:0:0","tags":["Diary"],"title":"See ya, 2020","uri":"/zh-cn/diary/see-ya-2020/"},{"categories":["Learn"],"content":"Lecture by Andrew Ng, Coursera ","date":"2020-12-18","objectID":"/zh-cn/machine-learning/:0:0","tags":["AI"],"title":"Machine Learning","uri":"/zh-cn/machine-learning/"},{"categories":["Learn"],"content":"Week 1 ","date":"2020-12-18","objectID":"/zh-cn/machine-learning/:1:0","tags":["AI"],"title":"Machine Learning","uri":"/zh-cn/machine-learning/"},{"categories":["Learn"],"content":"Supervised Learning vs. Unsupervised Learning Supervised Learning: In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output. Training Data already has the value, which our Function should predict for a new, strange input. i.e, In the case below, we want to get a function, so that we can calculate the price of a house to be sold. We have 4 Training Data in this table and each Data has 4 Features that have a influence on its' price ( we call it as Multi-Features. It’ll be discussed later:) ). It’s devided in Regression and Classification Problems Size Number of Bedrooms Number of Floors Age(years) Price(1000$) $TrainingData_1$ 2104 5 1 45 460 $TrainingData_2$ 1416 3 2 40 232 $TrainingData_3$ 1534 3 2 30 315 $TrainingData_4$ 852 2 1 36 178 … … … … … … ($x_1$) ($x_2$) ($x_3$) ($x_4$) ($x_5$) Unsupervised Learning Unsupervised learning, on the other hand, allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don’t necessarily know the effect of the variables. ","date":"2020-12-18","objectID":"/zh-cn/machine-learning/:1:1","tags":["AI"],"title":"Machine Learning","uri":"/zh-cn/machine-learning/"},{"categories":["Learn"],"content":"Regression vs. Classification Regression: The Value we want to predict is continious instead of discrete. i.g., the Price of a House. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. Classification: As its' name, we want to devide the input in different classes. i.g., After training, with the help of the color, weight, outlook and smel ( Features ), we want to predict if the coffee beans we have come from Arabica or Robusta. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories. ","date":"2020-12-18","objectID":"/zh-cn/machine-learning/:1:2","tags":["AI"],"title":"Machine Learning","uri":"/zh-cn/machine-learning/"},{"categories":["Learn"],"content":"Linear Regression with 1 Variable / Feature Hypothesis Function $\\hat y = h_\\theta = \\theta_0 + \\theta_1x$ Cost Function $J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i=1}^m(\\hat y_i - y_i)^2 = \\frac{1}{2m} \\sum_{i=1}^m(h_\\theta(x_i) - y_i)^2$ Optimize Gradient Descnet. More specifically, repeat: $tmp_0 = \\theta_0 - \\alpha\\frac{\\partial}{\\theta_0}{J(\\theta_0, \\theta_1)} = \\theta_0 - \\alpha\\frac{1}{m}\\sum_{i=1}^m{(h_\\theta(x_i) - y_i)}$ $tmp_1 = \\theta_1 - \\alpha\\frac{\\partial}{\\theta_1}{J(\\theta_0, \\theta_1)} = \\theta_1 - \\alpha\\frac{1}{m}\\sum_{i=1}^m{((h_\\theta(x_i) - y_i)x_1)}$ $\\theta_0 = tmp_0$ $\\theta_1 = tmp_1$ We call $\\alpha$ Learning Rate. Its' value has influence on the speed, how fast we can find the parameters, with that we can reach a local optimal value. When $\\alpha$ too large / too small is: too large: we might miss the parameters, which can help us get the local optimal value. too small: We need more iterations ( Baby Steps / more time ). We can make the judge, whether $\\alpha$ too large / small ist by drawing the plot of iterations - $J(\\theta_0, \\theta_1)$: It should be lower. Otherwise is $\\alpha$ too large. We can start $\\alpha$ from 0.001, …, 0.01, …, 0.1. And Andrew Ng advises us to use 3 times ( 0.001, 0.003, 0.01, 0.03, 0.1 ) to try it. ","date":"2020-12-18","objectID":"/zh-cn/machine-learning/:1:3","tags":["AI"],"title":"Machine Learning","uri":"/zh-cn/machine-learning/"},{"categories":["Learn"],"content":"Multiple Features and Matrix Vector: A vector is a Matrix with one Column and many rows. i.g., $\\begin{bmatrix}x_0 \\ x_1 \\x_2 \\x_3 \\end{bmatrix}$ Hypothesis: We use the table above as example, we have 4 Training Data and each of them has 4 Features. According to this, we should have 4+1 parameters $\\theta_0-\\theta_4$. We write the Hypothesis below: $h_\\theta(x^i) = \\theta_0 + \\sum_{j=1}^4\\theta_jx_j^i$ $x_i^j$ here means the jth Feature of the ith Training Data. Actually we can add a Feature $x_0 \\equiv 1$, then we can use the Matrix. So that we can write the Equation above as below: $h_\\theta(x^i) = \\sum_{j=0}^4\\theta_jx_j^i$ We let: $\\Theta = \\begin{bmatrix}\\theta_0, \\theta_1, \\theta_2, \\theta_3, \\theta_4\\end{bmatrix}$, $X = \\begin{bmatrix} x^1_0 \u0026 x^2_0 \u0026 x^3_0 \u0026 x^4_0 \\\\ x^1_1 \u0026 x^2_1 \u0026 x^3_1 \u0026 x^4_1 \\\\ x^1_2 \u0026 x^2_2 \u0026 x^3_2 \u0026 x^4_2 \\\\ x^1_3 \u0026 x^2_3 \u0026 x^3_3 \u0026 x^4_3 \\\\ x^1_4 \u0026 x^2_4 \u0026 x^3_4 \u0026 x^4_4 \\end{bmatrix}$. So that $h_\\theta(x^i) = \\Theta * i_{th}$ column of $X$. In Matrix $X$, element $x_0^i \\equiv 1$. We can also calculate $\\Theta * X$ directly, the $i_{th}$ colum is the value of $h_\\theta(x^i)$. Cost Function: Assume we have m Training Data, each of them has n Features. $J(\\theta_0, \\theta_1, … \\theta_n) = \\frac{1}{2m}\\sum_{i=1}^m(h_\\theta(x^i) - y^i)^2$ Gradien Descent: repeat: $tmp_i = \\theta_i - \\alpha\\frac{\\partial}{\\partial\\theta_i}J(\\theta_0, \\theta_1, …, \\theta_n) = \\theta_i - \\alpha\\frac{1}{m}\\sum_{i=1}^n(h_\\theta(x^i) - y^i)* x^i)$ $\\theta_i = tmp_i$ ","date":"2020-12-18","objectID":"/zh-cn/machine-learning/:1:4","tags":["AI"],"title":"Machine Learning","uri":"/zh-cn/machine-learning/"},{"categories":["Learn"],"content":"Feature Scaling We use this technique when the range of Features have great Difference. i.g., $0 \\leq x_1 \\leq 2000, 0 \\leq x_2 \\leq 5$, then we can $x_1 \\coloneqq x_1 / 2000, x_2 \\coloneqq x_2 / 5.$ This helps with the speed up of the Gradient Descent. Andrew Ng advices that scaling all Features approximately into the range [-1, 1]. We can also scale the Features by using this: $x_i = \\frac{x_i - \\mu_i}{s_i}$ Here $\\mu_i$ is the average value of Feature $x_i$, $s_i$ is normally $value_{max} - value_{min}$. ","date":"2020-12-18","objectID":"/zh-cn/machine-learning/:1:5","tags":["AI"],"title":"Machine Learning","uri":"/zh-cn/machine-learning/"},{"categories":["Learn"],"content":"Normal Equation The Method to solve for $\\Theta$ analytically: $\\Theta = (X^TX)^{-1}X^Ty$ can minimize the $J(\\theta)$. $x^i = \\begin{bmatrix}x_0^i, x_1^i, …, x_n^i\\end{bmatrix}$ $X = \\begin{bmatrix} x_0^1 \u0026 x_1^1 \u0026 {…} \u0026 x_n^1 \\\\ x_0^2 \u0026 x_1^2 \u0026 {…} \u0026 x_n^2 \\\\ … \u0026 … \u0026 … \u0026 … \\\\x_0^m \u0026 x_1^m \u0026 {…} \u0026 x_n^m \\end{bmatrix}$ In this Matrix, we write each row the Features of one Training Data. ","date":"2020-12-18","objectID":"/zh-cn/machine-learning/:1:6","tags":["AI"],"title":"Machine Learning","uri":"/zh-cn/machine-learning/"}]