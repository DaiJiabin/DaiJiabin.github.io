[{"categories":null,"content":"2021.03.13 梦到了一位身患绝症的姑娘。 她的父亲是一位南方的生意人，开了一家商场。商场两侧装潢风格一面是她的父亲选择的，另一面是她选择的。 她的父亲有一位很靠谱的司机，但我记不起自己是如何认识她的父亲的。有一天下着很大的雨，驾驶位是空的，车门也大开，他们父女两就坐在后座等司机回来。 我笑说我也会开车啊，就钻进去（是一辆现代）横冲直撞，把车刮了两次，但也开回了小区（车的离合，刹车不好用）。 我看到了这位姑娘在破站上的账号，账号昵称是很有特色的颜文字（狗头，健壮的双臂，一只手上托着一坨便便）。在动态里我看到了相当多可以称得上是私密的事情：包括她已经病入膏肓。 有一条动态清楚地表明，她的父亲已经放弃继续投资她的治疗：他让她选择自己裹尸袋的颜色。 她的父亲同他说：如果你下得了手术台，并且无限温柔，这个话题就暂且不谈；如果没有，那就是黑色的。 另外一种情况是什么，我忘记了，但对应的裹尸袋是黄色的。 然而我也能从动态里看出来， 她依然希望可以活下去。 正在我看她这些动态的时候，她来找我，嗔我没回她微信。退回微信一看，她发来一张照片：自封袋装着我的一缕头发和一个她买的形状很奇怪的修正带（有两个头，像一个烤肉夹）。 她笑着同我讲：你们北方人剪个头发还这么讲究，剪完都不能随便乱扔的？ 我答非所问：我要走了——我已经离家太久，我的妹妹很想我。 她表示理解。 我说：走之前可以抱我一下吗？ 回了家，我在床上醒来，甚至在破站上她的动态里刷到了她父亲对未来女婿的期许：婚礼必须是国宴级别，彩礼28w，但不必要有车。 我感觉到她是一个很绝望的人。 有一条她的动态，是一只用各种符号、线条组成的小老鼠。画面左上角——也就是小老鼠的右耳，是一只带着红色火苗的小蜡烛。 过几天再看，小老鼠通体变成了红色，唯独蜡烛的火苗变成了白色。 我知道那位姑娘已经离开了。 虽然只是梦里的爱而不得，但醒来还是很失落。 ","date":"2021-03-13","objectID":"/zh-cn/diary/dreams/:1:0","tags":["Diary"],"title":"Dreams","uri":"/zh-cn/diary/dreams/"},{"categories":null,"content":"2021.03.14 梦到被窝里有很多蛇，但有的友善，有的凶猛。 掐死了一只露出獠牙扑向我的赤红的一条。 ","date":"2021-03-13","objectID":"/zh-cn/diary/dreams/:2:0","tags":["Diary"],"title":"Dreams","uri":"/zh-cn/diary/dreams/"},{"categories":null,"content":"2021 ","date":"2021-02-25","objectID":"/zh-cn/diary/02.2021/:0:0","tags":["Diary"],"title":"02.2021","uri":"/zh-cn/diary/02.2021/"},{"categories":null,"content":"2月 ","date":"2021-02-25","objectID":"/zh-cn/diary/02.2021/:1:0","tags":["Diary"],"title":"02.2021","uri":"/zh-cn/diary/02.2021/"},{"categories":null,"content":"24.02.2021 我的父母，他们真的很伟大。 或许我这辈子都无法成为伟大无私及他们一半的人。 他们善良，他们朴素。 上帝，请你对他们好一些。 ","date":"2021-02-25","objectID":"/zh-cn/diary/02.2021/:1:1","tags":["Diary"],"title":"02.2021","uri":"/zh-cn/diary/02.2021/"},{"categories":null,"content":"This is a Test File… But I’m sure I’ll write it :) . 20.12.2020 一语盖之，“魔幻”。 新冠疫情八千公里开外肆虐作序，学业和生活受到愈来愈大的影响， 这都是在搭上出国那趟航班前想都不敢想的事情。在那之前所有的憧憬，哪怕是认识几个对中国文化感兴趣的外国同学，都瞬间化作了泡影。 但似乎也没什么大不了的。只是有时候认真的回想，不带口罩的日子，还有临别前的握手、拥抱，是很遥远的日子了。去年的这个时候，疫情尚未波及德国，还有空气中满溢着Gluehwein香甜气息的圣诞市场。活在当下真的是一个很重要的教训……去年没有好好体验这边的圣诞节，今年街道又冷清如同死城——只能等明年了。 只希望疫情能尽快过去。 “愿世界和平”从调侃变成了很正经的一句话。有种说不上来的滋味。 31.12.2020 ","date":"2020-12-19","objectID":"/zh-cn/diary/see-ya-2020/:0:0","tags":["Diary"],"title":"See ya, 2020","uri":"/zh-cn/diary/see-ya-2020/"},{"categories":["Learn"],"content":"Lecture by Andrew Ng, Coursera ","date":"2020-12-18","objectID":"/zh-cn/machine-learning/:0:0","tags":["AI"],"title":"Machine Learning","uri":"/zh-cn/machine-learning/"},{"categories":["Learn"],"content":"Week 1 ","date":"2020-12-18","objectID":"/zh-cn/machine-learning/:1:0","tags":["AI"],"title":"Machine Learning","uri":"/zh-cn/machine-learning/"},{"categories":["Learn"],"content":"Supervised Learning vs. Unsupervised Learning Supervised Learning: In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output. Training Data already has the value, which our Function should predict for a new, strange input. i.e, In the case below, we want to get a function, so that we can calculate the price of a house to be sold. We have 4 Training Data in this table and each Data has 4 Features that have a influence on its' price ( we call it as Multi-Features. It’ll be discussed later:) ). It’s devided in Regression and Classification Problems Size Number of Bedrooms Number of Floors Age(years) Price(1000$) $TrainingData_1$ 2104 5 1 45 460 $TrainingData_2$ 1416 3 2 40 232 $TrainingData_3$ 1534 3 2 30 315 $TrainingData_4$ 852 2 1 36 178 … … … … … … ($x_1$) ($x_2$) ($x_3$) ($x_4$) ($x_5$) Unsupervised Learning Unsupervised learning, on the other hand, allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don’t necessarily know the effect of the variables. ","date":"2020-12-18","objectID":"/zh-cn/machine-learning/:1:1","tags":["AI"],"title":"Machine Learning","uri":"/zh-cn/machine-learning/"},{"categories":["Learn"],"content":"Regression vs. Classification Regression: The Value we want to predict is continious instead of discrete. i.g., the Price of a House. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. Classification: As its' name, we want to devide the input in different classes. i.g., After training, with the help of the color, weight, outlook and smel ( Features ), we want to predict if the coffee beans we have come from Arabica or Robusta. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories. ","date":"2020-12-18","objectID":"/zh-cn/machine-learning/:1:2","tags":["AI"],"title":"Machine Learning","uri":"/zh-cn/machine-learning/"},{"categories":["Learn"],"content":"Linear Regression with 1 Variable / Feature Hypothesis Function $\\hat y = h_\\theta = \\theta_0 + \\theta_1x$ Cost Function $J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i=1}^m(\\hat y_i - y_i)^2 = \\frac{1}{2m} \\sum_{i=1}^m(h_\\theta(x_i) - y_i)^2$ Optimize Gradient Descnet. More specifically, repeat: $tmp_0 = \\theta_0 - \\alpha\\frac{\\partial}{\\theta_0}{J(\\theta_0, \\theta_1)} = \\theta_0 - \\alpha\\frac{1}{m}\\sum_{i=1}^m{(h_\\theta(x_i) - y_i)}$ $tmp_1 = \\theta_1 - \\alpha\\frac{\\partial}{\\theta_1}{J(\\theta_0, \\theta_1)} = \\theta_1 - \\alpha\\frac{1}{m}\\sum_{i=1}^m{((h_\\theta(x_i) - y_i)x_1)}$ $\\theta_0 = tmp_0$ $\\theta_1 = tmp_1$ We call $\\alpha$ Learning Rate. Its' value has influence on the speed, how fast we can find the parameters, with that we can reach a local optimal value. When $\\alpha$ too large / too small is: too large: we might miss the parameters, which can help us get the local optimal value. too small: We need more iterations ( Baby Steps / more time ). We can make the judge, whether $\\alpha$ too large / small ist by drawing the plot of iterations - $J(\\theta_0, \\theta_1)$: It should be lower. Otherwise is $\\alpha$ too large. We can start $\\alpha$ from 0.001, …, 0.01, …, 0.1. And Andrew Ng advises us to use 3 times ( 0.001, 0.003, 0.01, 0.03, 0.1 ) to try it. ","date":"2020-12-18","objectID":"/zh-cn/machine-learning/:1:3","tags":["AI"],"title":"Machine Learning","uri":"/zh-cn/machine-learning/"},{"categories":["Learn"],"content":"Multiple Features and Matrix Vector: A vector is a Matrix with one Column and many rows. i.g., $\\begin{bmatrix}x_0 \\ x_1 \\x_2 \\x_3 \\end{bmatrix}$ Hypothesis: We use the table above as example, we have 4 Training Data and each of them has 4 Features. According to this, we should have 4+1 parameters $\\theta_0-\\theta_4$. We write the Hypothesis below: $h_\\theta(x^i) = \\theta_0 + \\sum_{j=1}^4\\theta_jx_j^i$ $x_i^j$ here means the jth Feature of the ith Training Data. Actually we can add a Feature $x_0 \\equiv 1$, then we can use the Matrix. So that we can write the Equation above as below: $h_\\theta(x^i) = \\sum_{j=0}^4\\theta_jx_j^i$ We let: $\\Theta = \\begin{bmatrix}\\theta_0, \\theta_1, \\theta_2, \\theta_3, \\theta_4\\end{bmatrix}$, $X = \\begin{bmatrix} x^1_0 \u0026 x^2_0 \u0026 x^3_0 \u0026 x^4_0 \\\\ x^1_1 \u0026 x^2_1 \u0026 x^3_1 \u0026 x^4_1 \\\\ x^1_2 \u0026 x^2_2 \u0026 x^3_2 \u0026 x^4_2 \\\\ x^1_3 \u0026 x^2_3 \u0026 x^3_3 \u0026 x^4_3 \\\\ x^1_4 \u0026 x^2_4 \u0026 x^3_4 \u0026 x^4_4 \\end{bmatrix}$. So that $h_\\theta(x^i) = \\Theta * i_{th}$ column of $X$. In Matrix $X$, element $x_0^i \\equiv 1$. We can also calculate $\\Theta * X$ directly, the $i_{th}$ colum is the value of $h_\\theta(x^i)$. Cost Function: Assume we have m Training Data, each of them has n Features. $J(\\theta_0, \\theta_1, … \\theta_n) = \\frac{1}{2m}\\sum_{i=1}^m(h_\\theta(x^i) - y^i)^2$ Gradien Descent: repeat: $tmp_i = \\theta_i - \\alpha\\frac{\\partial}{\\partial\\theta_i}J(\\theta_0, \\theta_1, …, \\theta_n) = \\theta_i - \\alpha\\frac{1}{m}\\sum_{i=1}^n(h_\\theta(x^i) - y^i)* x^i)$ $\\theta_i = tmp_i$ ","date":"2020-12-18","objectID":"/zh-cn/machine-learning/:1:4","tags":["AI"],"title":"Machine Learning","uri":"/zh-cn/machine-learning/"},{"categories":["Learn"],"content":"Feature Scaling We use this technique when the range of Features have great Difference. i.g., $0 \\leq x_1 \\leq 2000, 0 \\leq x_2 \\leq 5$, then we can $x_1 \\coloneqq x_1 / 2000, x_2 \\coloneqq x_2 / 5.$ This helps with the speed up of the Gradient Descent. Andrew Ng advices that scaling all Features approximately into the range [-1, 1]. We can also scale the Features by using this: $x_i = \\frac{x_i - \\mu_i}{s_i}$ Here $\\mu_i$ is the average value of Feature $x_i$, $s_i$ is normally $value_{max} - value_{min}$. ","date":"2020-12-18","objectID":"/zh-cn/machine-learning/:1:5","tags":["AI"],"title":"Machine Learning","uri":"/zh-cn/machine-learning/"},{"categories":["Learn"],"content":"Normal Equation The Method to solve for $\\Theta$ analytically: $\\Theta = (X^TX)^{-1}X^Ty$ can minimize the $J(\\theta)$. $x^i = \\begin{bmatrix}x_0^i, x_1^i, …, x_n^i\\end{bmatrix}$ $X = \\begin{bmatrix} x_0^1 \u0026 x_1^1 \u0026 {…} \u0026 x_n^1 \\\\ x_0^2 \u0026 x_1^2 \u0026 {…} \u0026 x_n^2 \\\\ … \u0026 … \u0026 … \u0026 … \\\\x_0^m \u0026 x_1^m \u0026 {…} \u0026 x_n^m \\end{bmatrix}$ In this Matrix, we write each row the Features of one Training Data. ","date":"2020-12-18","objectID":"/zh-cn/machine-learning/:1:6","tags":["AI"],"title":"Machine Learning","uri":"/zh-cn/machine-learning/"}]